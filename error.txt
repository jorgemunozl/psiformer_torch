The logged NaNs come straight from the energy term you average in train.py (lines 59-72).
 E_mean and loss are just statistics of local_energies, so once any local energy is invalid,
  everything that follows turns into NaN.

Every local energy is produced by Hamiltonian.local_energy (src/psiformer_torch/hamiltonian.py
 (lines 36-44)). That routine differentiates log ψ once (grad_log_psi, lines 46-57) and again when
  computing the Laplacian (lines 59-81). Because it retains the graph, the Laplacian
   directly asks PyTorch to take second derivatives through every operation that appears in your wave
    function.

The wave function itself is dominated by the Slater determinants built in Orbital_Head.slogdet_sum
 (src/psiformer_torch/psiformer.py (lines 129-184)). At initialization those determinant blocks are
  essentially random. Whenever any of the per-spin matrices becomes singular (determinant exactly 
  zero), torch.linalg.slogdet returns (sign=0, logabsdet=-inf). Lithium’s spin‑down block is only 1×1
  , so it becomes zero whenever the corresponding linear head output is zero or the envelope 
  suppresses it. The moment that happens you pass -inf back into the Hamiltonian and immediately
   request first and second derivatives of that value.

Backpropagating through logabsdet=-inf is undefined. PyTorch responds by filling the gradient/
Laplacian tensors with NaNs, which propagate through kinetic and V in hamiltonian.py (lines 36-44), 
then through E_mean/loss in train.py (lines 68-81), giving the NaN logs you’re seeing as early as
 step 0.

What to do next:

Instrument PsiFormer.forward to assert torch.isfinite(logdet_up) and torch.isfinite(logdet_down)
 before you add the Jastrow term. You’ll see the first NaNs/±inf arise there.
Add a small regularization when forming the determinants (e.g., add eps*I before torch.linalg.
slogdet, or clamp the envelope so entries cannot drive the matrix to zero) and keep track of the
 sign instead of discarding it. This keeps the matrices invertible so the derivatives requested 
 in hamiltonian.py stay finite.
Until the determinants are numerically safe, guard the sampling loop by skipping Monte Carlo samples
 with non‑finite log ψ to keep the chain (and the optimizer) from being poisoned.
Once you stop torch.linalg.slogdet from ever seeing singular matrices, the Hamiltonian call will
 return finite energies and the NaNs in the logs disappear.