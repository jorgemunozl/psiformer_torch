\documentclass{beamer}

\mode<presentation>
{
\usetheme{Madrid}
\usecolortheme{default}
\usefonttheme{serif}
\setbeamertemplate{navigationsymbols}{}
\setbeamertemplate{caption}[numbered]
}
\AtBeginSection[]{
  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}


\setbeamertemplate{footline}{%
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.5ex,dp=1ex,left]{author in head/foot}%
      \hspace*{1ex}\insertshortauthor
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.55\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
      Schrodinger Equation with Transformers
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.13\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
      \insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
    \end{beamercolorbox}%
  }%
}
\setbeamertemplate{navigation symbols}{} % usually nicer without icons



\usepackage{amsmath,amssymb,bm,mathtools,physics,siunitx,tikz,xcolor,graphicx,hyperref}
\title{Teaching Quantum Chemistry to a Deep Learning Model }

\subtitle{Transformers for the many body Schrodinger Equation}
\author{Jorge Munoz Laredo, Angel Flores, Daniel Paredes}
\date{\today}
\begin{document}
\begin{frame}
    \titlepage
\end{frame}


\section{The Schr\"odinger Wave Function and the physical laws that rule}

\subsection{Schr\"odinger Equation}

\begin{frame}{The Schr\"odinger equation}
  On 1926 Schrodinger derived his equatin:
\begin{equation}
\hat{H}\,\Psi=E \Psi
\end{equation}

\begin{itemize}
\item $\Psi$ is a complex value function called \textbf{wave function}.
\item $\hat{H}$ is called the \textbf{Hamiltonian Operator}.
\end{itemize}

\begin{block}{Hamiltonian}
\begin{equation}
\hat{H}=\frac{\hat{\vec{{P}}}^{2}}{2m}+\hat{V} =-\frac{\hbar^{2}}{2m}\nabla^{2}+\hat{V}
\end{equation}
- Find the electrostatic potential $V$ of the system.
\end{block}
\end{frame}

\begin{frame}{Many-Body System}
When considering $n$ bodies, we have:
\begin{equation}
\hat{H}\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})=E\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})
\end{equation}

With $\mathbf{x}_{i}=\{ \mathbf{r}_{i},\sigma \}$, where $\mathbf{r}_{i}\in \mathbb{R}^{3}$ is the position of each particle and $\sigma \in \{ \uparrow.\downarrow \}$ is the so called spin.

\begin{alertblock}{Considerations}
 \begin{itemize}
  \item Each particle interact with all the another particles.
  \item For atoms, consider all the protons, electrons and neutrons.
  \item Solution obey physical laws.
\end{itemize}
\end{alertblock}
\end{frame}
\begin{frame}{Setting up the Hamiltonian}
  The first step is obtain a practical form of the \textbf{Hamiltonian}.
\begin{itemize}
        \item Kinetic energy: $T = -\frac{1}{2}\sum_{i=1}^{N} \nabla_i^2$.
        \item Electron-electron repulsion: $V_{ee} = \sum_{i<j} \frac{1}{r_{ij}}$.
    \end{itemize}
\begin{align}
   \hat H  =
-\sum_{i=1}^{N}\frac{1}{2}\nabla_i^{2}
-\sum_{i=1}^{N}\frac{Z_I}{|\mathbf r_i-\mathbf R_I|}
 +\sum_{1\le i<j\le N}\frac{1}{|\mathbf r_i-\mathbf r_j|}
\end{align}

\end{frame}

\subsection{Physical laws and conditions}

\begin{frame}{Fermi-Dirac statistics}
All the fermions follow the Fermi-Dirac Statistics, this is.
 \begin{itemize}
        \item Exchanging two electrons flips the wavefunction's sign: $\Psi(\dots i,j \dots) = -\,\Psi(\dots j,i \dots)$.
 \end{itemize}

 \begin{block}{Slater Determinant}
Enforce it using a determinant.
\begin{equation}
\psi=
\begin{vmatrix}
\phi_{1}^{k}(\mathbf{x}_{1})  & \dots  &  \phi_{1}^{k}(\mathbf{x}_{n}) \\
\vdots   &  & \vdots  \\
\phi_{n}^{k}(\mathbf{x}_{1}) & \dots & \phi_{n}^{k}(\mathbf{x}_{n})
\end{vmatrix}
\end{equation}
\end{block}
Where $\phi$ are called spin orbitals
\end{frame}

\begin{frame}{Kato cusp conditions, Jastrow Factor}
The potential are:
 $$\sum\frac{1}{|\mathbf r_i-\mathbf r_j|}$$
\begin{itemize}
\item Coulomb potentials cause a sharp cusp in $\Psi$ when particles overlaps.
    \end{itemize}
\begin{block}{Jastrow Factor $\exp(\mathcal{J})$}
In this work we are going to use this specific form:
\begin{equation}
 \mathcal{J}_{\theta}(x)=\sum_{i<j;\sigma_{i}=\sigma_{j}}-\frac{1}{4}\frac{\alpha^{2}_{par}}{\alpha_{par}+\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert }+\sum_{i,j;\sigma_{i}\neq \sigma_{j}}-\frac{1}{2}\frac{\alpha^{2}_{anti}}{\alpha_{anti}+\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert }
\end{equation}
\end{block}
\end{frame}

\subsection{Optimizing an Ansatz}


\begin{frame}

\frametitle{Loss: Variational Principle}

Variational principle states:

$$E[\Psi] = \displaystyle \frac{\langle \Psi \mid H \mid \Psi \rangle}{\langle \Psi \mid \Psi \rangle} \ge E_0$$

Minimizing $E[\Psi]$ drives the ansatz toward the ground state.
$$
E[\Psi]=\mathcal{L}(\Psi_\theta)=\frac{\bra{\Psi_{\theta}} \hat{H}\ket{\Psi_{\theta}} }{\braket{ \Psi_{\theta}} }=\frac{\int d\mathbf{R}\Psi ^{*}(\mathbf{R})\hat{H}\Psi(\mathbf{R})}{\int d\mathbf{R}\Psi ^{*}(\mathbf{R})\Psi(\mathbf{R})}
$$
Define:
$$p_{\theta}(\mathbf{R})=|\Psi_{\theta}(\mathbf{R})|^{2}\frac{1}{\int  d\mathbf{R}'\Psi^{2}_{\theta}(\mathbf{R}')}\land E_{L}(\mathbf{R})=\Psi ^{-1}_{\theta}(\mathbf{R})\hat{H}\Psi_{\theta}(\mathbf{R})
$$
Then:
\begin{equation}
\mathcal{L}_{\theta}=\mathbb{E}_{\mathbf{R}\sim p_{\theta}}[E_{L}(\mathbf{R})]
\end{equation}
\end{frame}

\begin{frame}{Variational Monte Carlo}
  \begin{block}{Quantum Monte Carlo}
With the samples $\mathbf{R}_{1},\dots,\mathbf{R}_{M}\sim p_{\theta}(\mathbf{R})$ we can make:

\begin{equation}
\mathcal{L}_{\theta}=\mathbb{E}_{x\sim p_{\theta}}[E_{L}(x)]\approx \frac{1}{M}\sum_{i=1}^{M} E_{L}(\mathbf{R}_{k})
\end{equation}
\end{block}
With:
$$E_{L}(\mathbf{R}_{k})=\frac{\hat{H}\psi(\mathbf{R}_{k})}{\psi(\mathbf{R}_{k})}=-\frac{1}{2}\frac{\nabla^{2}\psi(\mathbf{R_{k}})}{\psi(\mathbf{R}_{k})}+V(\mathbf{R}_{k})
$$
\textbf{Obtain $\mathbf{R}_k$} $\to$ \textbf{Metropolis-Hastings Algorithm}
\end{frame}
\begin{frame}
  \frametitle{Metropolis-Hastings Algorithm}
\textbf{Goal: Generate many samples} $\mathbf{R}\sim \rho$, Requirement: $C\rho$

1. $\mathbf{X}_{0}\in E$ arbitrary:

2. Propose $\mathbf{X}'=\mathbf{X}_{0}+\eta$ ,where $\eta \sim q(\eta)$, (Normal Gaussian)

3. Compute the quantity:
$$
A(\mathbf{X_{0}}, \mathbf{X}')=\text{min}\left( 1,\frac{\rho(\mathbf{X}')}{\rho(\mathbf{X}_{0})} \right)
$$
4. Generate a uniform number $U\in[0,1]$. If: $U<A(\mathbf{X}_{0} ,\mathbf{X'})$ then $\mathbf{X_{1}}=\mathbf{X}'$, otherwise try another $\mathbf{X}'$. Accept or decline.

\end{frame}

\begin{frame}{Metropolis-Hastings Algorithm}
  \begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{img/mcmc.png}
    \caption{Metropolis-Hastings Walkers}
    \label{fig:placeholder}
  \end{figure}
The model is able to obtain its own data (no dataset $\mathcal{D}$), able to compute:
$$ \mathcal{L}_{\theta}=\mathbb{E}_{\mathbf{R}\sim |\Psi_{\theta}|^{2}}[E_{L}(\mathbf{R})]$$,
\end{frame}

\begin{frame}{Loss function problems}
$$ \mathcal{L}_{\theta}=\mathbb{E}_{\mathbf{R}\sim |\Psi_{\theta}|^{2}}[E_{L}(\mathbf{R})]$$,
\begin{itemize}
\item Three Backpropagations
\item $|\Psi_{\theta}|^{2}$ changes over time.
\item You are just optimizing just the \textbf{energy}, not the wave function itself.
\end{itemize}
Solution: \textbf{Log Derivative Trick}

$$\qquad \, \nabla _{\theta}\mathcal{L}=2\mathbb{E}_{\mathbf{R}\sim \Psi^{2}}[ (E_{L}(\mathbf{R})-\mathbb{E}_{p}[E_L])\nabla _{\theta}\log \psi]$$
REINFORCE!
$$
\nabla_{\theta}J(\theta)=\mathbb{E}_{\tau \sim \pi}[({R(\tau)}-b)\nabla_{\theta}\log \pi_{\theta}(\tau)]
$$
\end{frame}

% ------------------------------
\section{Transformers}
\subsection{Attention Mechanism}

\begin{frame}{Transformer Architecture}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{}
    \caption{Caption}
  \end{figure}
\end{frame}

\begin{frame}
\frametitle{Attention on the room}
\textbf{Multi Head Attention}
$d$ the embedding dimension, $n_h$ the number of attention heads, $d_h$ the dimension per head, and $\mathbf{h}_t \in \mathbb{R}^{d}$ the hidden dimension.
The learnable matrices are:
$$ W ^{Q}, W ^{K}, W ^{V} \in \mathbb{R}^{d_h n_h \times d}$$
$$ \mathbf{k}_{i}=\mathbf{W}^{k}\mathbf{h}_{i},\mathbf{q}_{i}=\mathbf{W}^{q}\mathbf{h}_{i},\mathbf{v}_{i}=\mathbf{W}^{v}\mathbf{h}_{i} $$

$$
\begin{align}
[\mathbf{q_{1},q_{2},\dots ,q_{n_{h}}}]=\mathbf{q} \\
[\mathbf{k_{1},k_{2},\dots ,k_{n_{h}}}]=\mathbf{k} \\
[\mathbf{v_{1},v_{2},\dots ,v_{n_{h}}}]=\mathbf{v}
\end{align}
$$
In the $i-th$ head:
\begin{equation}
\mathbf{o}_{t,i}=\sum_{j=1}^{t}\text{Softmax}\left( \frac{\mathbf{q}^{T}_{t,i}\mathbf{k}_{j,i}}{\sqrt{ d_{h} }} \right) \mathbf{v}_{j,i}
\end{equation}
$W^{O}\in \mathbb{R}^{d \times d_h n_h}$ the output projection matrix.
$$\mathbf{u}_{t}=W^{O}[\mathbf{o}_{t,1};\mathbf{o}_{t,2};\dots ;\mathbf{o}_{t,n_{h}}]$$
\end{frame}

\section{Fermi Net and Psiformer}
\subsection{Fermi Net}

\begin{frame}{Fermi Net Architecture}
    \begin{columns}

      \begin{column}{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=0.50\linewidth]{img/ferminet.png}
          \caption{Fermi Net Architecture}
          \end{figure}
      \end{column}

    \end{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item Learn the orbitals $\phi$
        \item Learn the coefficients.
      \end{itemize}
    \end{column}
\end{frame}

\begin{frame}
\frametitle{FermiNet baseline}
\begin{block}{Orbital}
FermiNet computes:
\begin{align}
	\psi(\mathbf{r}^\uparrow_1,\ldots,\mathbf{r}^\downarrow_{n^\downarrow}) =
  \sum_{k}\omega_k \det\left[\phi^{k \uparrow}_{ij} \right] \det\left[\phi^{k\downarrow}_{ij} \right]
\end{align}

\end{block}
\end{frame}

\subsection{Psi Former}

\begin{frame}{Psi Former Architecture}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=0.50\linewidth]{img/psiformer.png}
        \caption{Psi Former Architecture}
      \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item Learn the Jastrow factor $\mathcal{J}$
        \item Learn the coefficients $\omega$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\subsection{Practical Implementation Details}
\begin{frame}{Implementation : Laplacian Computation}
\end{frame}

\begin{frame}{Implementation : Determinant Stability}
\end{frame}

\begin{frame}{Implementation : Backpropagation on Markov Chain}
\end{frame}

\begin{frame}{Implementation : Optimizer Choice}
\end{frame}

\begin{frame}{Implementation : GPU Parallelization}
\end{frame}


\begin{frame}{Model Size}

\begin{figure}
  \centering
  \includegraphics[width=0.50\linewidth]{img/model_size.pdf}
  \caption{Model Size}
  \label{fig:model_size}
\end{figure}

\end{frame}

\begin{frame}{Results: Convergence Curve}
\begin{figure}
  \centering
  \includegraphics[width=0.50\linewidth]{img/convergence_curve.pdf}
  \caption{Convergence Curve}
  \label{fig:convergence_curve}
\end{figure}
\end{frame}

\begin{frame}{Results: Energy Estimates}

\begin{figure}
  \centering
  \includegraphics[width=0.50\linewidth]{img/energy_estimates.pdf}
  \caption{Energy Estimates}
  \label{fig:energy_estimates}
\end{figure}

\end{frame}

\begin{frame}{Improvements}
  \begin{itemize}
    \item Use a better optimizer.
    \item Use a better model.
    \item Use a better sampling method.
    \item Use a better model.
    \item Use a better model.
  \end{itemize}
\end{frame}

\begin{frame}{Thanks!}

\begin{figure}
  \centering
  \includegraphics[width=0.50\linewidth]{img/thanks.png}
  \end{figure}
\end{frame}
\end{document}
