\documentclass{beamer}

\mode<presentation>
{
\usetheme{Madrid}
\usecolortheme{default}
\usefonttheme{serif}
\setbeamertemplate{navigationsymbols}{}
\setbeamertemplate{caption}[numbered]
}
\AtBeginSection[]{
  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}


\usepackage{listings}
\usepackage{xcolor} % optional, but nice
\lstset{
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  showstringspaces=false
}



\setbeamertemplate{footline}{%
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.5ex,dp=1ex,left]{author in head/foot}%
      \hspace*{1ex}\insertshortauthor
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.55\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
      Schrodinger Equation with Transformers
    \end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.13\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
      \insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
    \end{beamercolorbox}%
  }%
}
\setbeamertemplate{navigation symbols}{} % usually nicer without icons



\usepackage{amsmath,amssymb,bm,mathtools,physics,siunitx,tikz,xcolor,graphicx,hyperref}
\title{Solving the Many-Electron Schr\"odinger Equation}
\subtitle{With a Transformer Architecture}
\author{Jorge Munoz Laredo}
\date{\today}
\begin{document}
\begin{frame}
    \titlepage
\end{frame}


%-----------------------

\section{The Schr\"odinger Wave Function and the physical laws that rule}

\subsection{Schr\"odinger Equation}

\begin{frame}{The Schr\"odinger equation}
  On 1926 Schrodinger derived the Time Dependent Form.(TDSE)

\begin{equation}
            i\,\hbar\,\partial_t \Psi = \hat{H}\,\Psi,
\end{equation}


\begin{itemize}
\item $\Psi \in \mathcal{H}$ is a complex value function called \textbf{wave function}.
\item $\hat{H}$ is called the \textbf{Hamiltonian Operator} , encodes all the information of the energy of the system.
\item Depends on the position $\vec{\mathbf{r}}$ of a particle and the temporal evolution $(t)$.
\item $\Psi$ encodes all information about the system; $|\Psi|^2$ gives a probability density that integrates to 1.
\end{itemize}

\begin{block}{Hamiltonian}
In the position basis:

\begin{equation}
\hat{H}=\frac{\hat{\vec{{P}}}^{2}}{2m}+\hat{V} =-\frac{\hbar^{2}}{2m}\nabla^{2}+\hat{V}
\end{equation}

\end{block}
\end{frame}

\begin{frame}{Time Dependent Form}
When the wave function could be written as:

\begin{equation}
\psi(\vec{\mathbf{r}},t)=R(\vec{\mathbf{r}})T(t)
\end{equation}

TDSE returns you that:

\begin{equation}
T(t)=e^{ -iEt/\hbar } \land \hat{H}R(\vec{\mathbf{r}})=ER(\vec{\mathbf{r}})
\end{equation}


Where $E$ is the total energy of the system.
The eigen-problem becomes obtain $R$ solving:
\begin{block}{Find a $\Psi$, such that:}
\begin{equation}
\left[-\frac{\hbar^{2}}{2m}\nabla^{2}+V(\mathbf r,t)\right]\psi(\vec{\mathbf{r}})= E \psi(\vec{\mathbf{r}})
\end{equation}

\end{block}
Find the potential $V$ of the system.
\end{frame}



\begin{frame}{Wave Function as Probability Density}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.55\textwidth]{img/prob.png}
    \caption{$\lvert \Psi (\mathbf{R}) \rvert ^{2} $ represent the probability to find a particle near the position $\mathbf{R}$.}
  \end{center}
\end{figure}


\end{frame}


\begin{frame}{Many-Body System}
When considering a many-body system, we need to consider the position of each electron like also the spin of it. When considering $n$ bodies, we have:
\begin{equation}
\hat{H}\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})=E\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})
\end{equation}

With $\mathbf{x}_{i}=\{ \mathbf{r}_{i},\sigma \}$, where $\mathbf{r}_{i}\in \mathbb{R}^{3}$ is the position of each particle and $\sigma \in \{ \uparrow.\downarrow \}$ is the so called spin.

\begin{alertblock}{Considerations}
 \begin{itemize}
  \item Each particle interact with all the another particles in specific ways.
  \item For atoms, consider all the protons, electrons and neutrons.
  \item Solution obey physical laws.
\end{itemize}
\end{alertblock}
\end{frame}

\begin{frame}{Setting up the Hamiltonian}
  The first step is obtain a practical form of the \textbf{Hamiltonian}.

\begin{itemize}
        \item Kinetic energy: $T = -\frac{1}{2}\sum_{i=1}^{N} \nabla_i^2$.
        \item Electron-nuclear attraction: $V_{en} = -\sum_{i,I} \frac{Z_I}{r_{iI}}$.
        \item Electron-electron repulsion: $V_{ee} = \sum_{i<j} \frac{1}{r_{ij}}$.
    \end{itemize}
\begin{align}
   \hat H & =
-\sum_{i=1}^{N}\frac{1}{2}\nabla_i^{2}
-\sum_{I=1}^{M}\frac{1}{2M_I}\nabla_{I}^{2}
-\sum_{i=1}^{N}\sum_{I=1}^{M}\frac{Z_I}{|\mathbf r_i-\mathbf R_I|} \notag\\
& +\sum_{1\le i<j\le N}\frac{1}{|\mathbf r_i-\mathbf r_j|}
+\sum_{1\le I<J\le M}\frac{Z_I Z_J}{|\mathbf R_I-\mathbf R_J|}
\end{align}

\textbf{Borh Oppenheimer} approximation helps with.
\end{frame}

\subsection{Physical laws and conditions}


\begin{frame}{Fermi-Dirac statistics}

All the fermions follow the Fermi-Dirac Statistics, this is.

  \begin{itemize}
        \item Electrons are indistinguishable fermions.
        \item Exchanging two electrons flips the wavefunction's sign: $\Psi(\dots i,j \dots) = -\,\Psi(\dots j,i \dots)$.
        \item Pauli exclusion: no two electrons can occupy the same
    \end{itemize}

    \begin{block}{Slater Determinant}
We can enforce this using a determinant to enforce an antisymmetric $\Psi$.
\begin{equation}
\psi=
\begin{vmatrix}
\phi_{1}^{k}(\mathbf{x}_{1})  & \dots  &  \phi_{1}^{k}(\mathbf{x}_{n}) \\
\vdots   &  & \vdots  \\
\phi_{n}^{k}(\mathbf{x}_{1}) & \dots & \phi_{n}^{k}(\mathbf{x}_{n})
\end{vmatrix}
\end{equation}
\end{block}
Where $\phi$ are called spin orbitals
\end{frame}

\begin{frame}{Kato cusp conditions, Jastrow Factor}

When two electrons

\begin{itemize}
        \item Coulomb potentials cause a sharp cusp in $\Psi$ when particles overlaps.
        \item Electron--nucleus cusp: $\displaystyle \frac{\partial \Psi}{\partial r_{iI}}\Big|_{r_{iI}=0} = -\,Z_I\,\Psi(0)$.
        \item Electron--electron cusp: $\displaystyle \frac{\partial \Psi}{\partial r_{ij}}\Big|_{r_{ij}=0} = \frac{1}{2}\,\Psi(0)$.
    \end{itemize}

\begin{block}{Jastrow Factor $\exp(\mathcal{J})$}
In this work we are going to use this specific form:


\begin{equation}
 \mathcal{J}_{\theta}(x)=\sum_{i<j;\sigma_{i}=\sigma_{j}}-\frac{1}{4}\frac{\alpha^{2}_{par}}{\alpha_{par}+\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert }+\sum_{i,j;\sigma_{i}\neq \sigma_{j}}-\frac{1}{2}\frac{\alpha^{2}_{anti}}{\alpha_{anti}+\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert }
\end{equation}

\end{block}
\end{frame}

\subsection{Optimizing an Ansatz}

\begin{frame}
\frametitle{Loss: Variational Principle}

Variational principle states:

$$E[\Psi] = \displaystyle \frac{\langle \Psi \mid H \mid \Psi \rangle}{\langle \Psi \mid \Psi \rangle} \ge E_0$$

Minimizing $E[\Psi]$ drives the ansatz toward the ground state.
$$
E[\Psi]=\mathcal{L}(\Psi_\theta)=\frac{\bra{\Psi_{\theta}} \hat{H}\ket{\Psi_{\theta}} }{\braket{ \Psi_{\theta}} }=\frac{\int d\mathbf{R}\Psi ^{*}(\mathbf{R})\hat{H}\Psi(\mathbf{R})}{\int d\mathbf{R}\Psi ^{*}(\mathbf{R})\Psi(\mathbf{R})}
$$
Define:
$$p_{\theta}(\mathbf{R})=|\Psi_{\theta}(\mathbf{R})|^{2}\frac{1}{\int  d\mathbf{R}'\Psi^{2}_{\theta}(\mathbf{R}')}\land E_{L}(\mathbf{R})=\frac{\hat{H}\Psi_{\theta}(\mathbf{R})}{\Psi_{\theta}(\mathbf{R})}
$$
Then:
\begin{equation}
\mathcal{L}_{\theta}=\mathbb{E}_{\mathbf{R}\sim |\Psi_{\theta}| ^{2}}[E_{L}(\mathbf{R})]
\end{equation}
\end{frame}
\begin{frame}{Variational Monte Carlo}
  \begin{block}{Quantum Monte Carlo}
With the samples $\mathbf{R}_{1},\dots,\mathbf{R}_{M}\sim |\Psi| ^{2}_{\theta}(\mathbf{R})$ we can make:

\begin{equation}
\mathcal{L}_{\theta}=\mathbb{E}_{\mathbf{R}\sim \Psi ^{2}_{\theta}}[E_{L}(\mathbf{R})]\approx \frac{1}{M}\sum_{i=1}^{M} E_{L}(\mathbf{R}_{k})
\end{equation}
\end{block}
With:
$$E_{L}(\mathbf{R}_{k})=\frac{\hat{H}\psi(\mathbf{R}_{k})}{\psi(\mathbf{R}_{k})}=-\frac{1}{2}\frac{\nabla^{2}\psi(\mathbf{R_{k}})}{\psi(\mathbf{R}_{k})}+V(\mathbf{R}_{k})
$$
\textbf{Obtain $\mathbf{R}_k$} $\to$ \textbf{Metropolis-Hastings Algorithm}
\end{frame}
\begin{frame}
  \frametitle{Metropolis-Hastings Algorithm}
\textbf{Goal: Generate many samples} $\mathbf{R}\sim \rho$, Requirement: $C\rho$

1. $\mathbf{R}_{0}\in E$ random.

2. Propose $\mathbf{R}'=\mathbf{R}_{0}+\eta$ ,where $\eta \sim q(\eta)$, (Normal Gaussian)

3. Compute the quantity:
$$
A(\mathbf{R_{0}}, \mathbf{R}')=\text{min}\left( 1,\frac{\rho(\mathbf{R}')}{\rho(\mathbf{R}_{0})} \right)
$$
4. Generate a uniform number $U\in[0,1]$. If: $U<A(\mathbf{R}_{0} ,\mathbf{R'})$ then $\mathbf{R_{1}}=\mathbf{R}'$, otherwise try another $\mathbf{R}'$. Accept or decline.

\end{frame}

\begin{frame}{Metropolis-Hastings}

In each sample generates $E_{L}(\mathbf{R}_{k})$ then average to obtain $\mathbb{E}(E_{L})$ and begin the back propagation step.

\begin{block}{Gradients of the Loss}
Using calculus you obtain:
\begin{equation}
  \nabla _{\theta}\mathcal{L}=2\mathbb{E}_{x\sim \Psi^{2}}[(E_{L}(x)-\mathbf{E}_{p}(E_L))\log \psi]
\end{equation}
\end{block}
This expectation is calculated in the same way.
\end{frame}


% ------------------------------



\begin{frame}{Transformer Architecture}
  \begin{columns}

    \begin{column}{0.5\textwidth}
      \begin{figure}
      \centering
      \includegraphics[width=0.56\textwidth]{img/psiformer_false_shapes.pdf}
      \caption{Tranformer backbone}
      \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \textbf{Multi Head Attention $\to$ Self Attention}
        \item $n_{\text{embd}}$ the embedding dimension
        \item $n_h$ the number of attention heads
        \item $d_h$ the dimension per head
        \item $\mathbf{h}_t \in \mathbb{R}^{ n_{\text{embd}} }$ the hidden dimension.
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
\frametitle{Attention on the room}
The learnable matrices are:
$$ W ^{Q}, W ^{K}, W ^{V} \in \mathbb{R}^{n_{\text{embd}}\times n_{\text{embd}}}$$
$$ \mathbf{k}_{i}=\mathbf{W}^{k}\mathbf{h}_{i},\mathbf{q}_{i}=\mathbf{W}^{q}\mathbf{h}_{i},\mathbf{v}_{i}=\mathbf{W}^{v}\mathbf{h}_{i} $$
$$
\begin{align}
[\mathbf{q_{1},q_{2},\dots ,q_{n_{h}}}]=\mathbf{q} \\
[\mathbf{k_{1},k_{2},\dots ,k_{n_{h}}}]=\mathbf{k} \\
[\mathbf{v_{1},v_{2},\dots ,v_{n_{h}}}]=\mathbf{v}
\end{align}
$$
In the $i-th$ head:
\begin{equation}
\mathbf{o}_{t,i}=\sum_{j=1}^{t}\text{Softmax}\left( \frac{\mathbf{q}^{T}_{t,i}\mathbf{k}_{j,i}}{\sqrt{ d_{h} }} \right) \mathbf{v}_{j,i}
\end{equation}
$W^{O}$ the output projection matrix.
$$\mathbf{u}_{t}=W^{O}[\mathbf{o}_{t,1};\mathbf{o}_{t,2};\dots ;\mathbf{o}_{t,n_{h}}]$$
\end{frame}

\section{Psiformer}
\subsection{Fermi Net}
\subsection{Psi Former}

\begin{frame}{Psiformer Ansatz}
\textbf{Ansatz:} Proposal model that you propose guided by intuition and that you optimize.
\begin{itemize}
    \item be \emph{antisymmetric} under particle exchange,
    \item capture strong \emph{electron--electron correlations}.
\end{itemize}


\textbf{Proposed ansatz}

Motivated by these constraints, we propose a Slater--Jastrow form:
\[
\Psi_{\theta}(\mathbf{R}) =
\underbrace{\exp\\big(\mathcal{J}_{\theta}(\mathbf{R})\big)}_{\text{Coulomb correlations}}
\,\times\,
\underbrace{\sum \omega_k  \det[ \boldsymbol{\phi}_{\theta}^{k}(\mathbf{R})]}_{\text{antisymmetry}}
\]
\end{frame}

\begin{frame}{Psi Former Architecture}
  \begin{columns} \begin{column}{0.5\textwidth} \begin{figure}
        \centering
        \includegraphics[width=0.70\linewidth]{img/psiformer.pdf}
        \caption{Psi Former Architecture}
      \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.37\textwidth]{img/legend.png}
\end{figure}
      \begin{itemize}
        \item The envelope is a matrix with learned exponential decay as elements.
        \item Learn the Jastrow factor only have two parameters.
        \item In practice we use logarithm scale.
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Psiformer Shapes}
  \begin{figure}
    \begin{center}
      \includegraphics[width=0.27\textwidth]{img/psiformer_true_shapes.pdf}
    \end{center}
    \caption{Psiformer shapes handling}\label{fig:}
  \end{figure}

\end{frame}

\begin{frame}{Methodology}
  \begin{columns}
  \begin{column}{0.5\textwidth}
\begin{figure}
    \centering
  \includegraphics[width=0.80\linewidth]{img/table_hyper.pdf}
  \caption{Psiformer Torch Small and Large}
\end{figure}
  \end{column}
  \begin{column}{0.5\textwidth}
    \begin{figure}
      \begin{center}
        \includegraphics[width=0.85\textwidth]{img/goal.png}
      \end{center}
    \end{figure}
    \begin{figure}
      \begin{center}
        \includegraphics[width=0.85\textwidth]{img/training.png}
      \end{center}
    \end{figure}
  \end{column}
\end{columns}
\end{frame}
\subsection{Practical Implementation Details}
\begin{frame}[fragile]{Implementation : Laplacian Computation}
The kinetic energy requires the Laplacian: $\nabla^2 \Psi(\mathbf{R}) = \sum_i \frac{\partial^2 \Psi}{\partial R_i^2}$
\begin{lstlisting}[language=Python]
R = R_o.requires_grad_(True)     # particle coordinates
psi = model(R)                # neural wavefunction
# first derivative: gradient
grad_psi = torch.autograd.grad(
    psi, R,
    create_graph=True,
    # retain_graph=True
)[0]
# second derivative: Laplacian
laplacian = 0.0
for i in range(R.shape[-1]):
    laplacian += torch.autograd.grad(
        grad_psi[..., i], R,
        # create_graph=True,
        retain_graph=True
    )[0][..., i]
\end{lstlisting}
\end{frame}

\begin{frame}{Determinant Stability in Psiformer}
\[
  \Psi_{\theta}(\mathbf{R}) \propto \det [\boldsymbol{\Phi}_{\theta}(\mathbf{R})]
\]
\textbf{Derivative of a determinant.}

$$
\frac{\partial \det(\mathbf{A})}{\partial \mathbf{A}}=\det(\mathbf{A})\mathbf{A}^{-T}
$$

\textbf{Key instability.}
If $\boldsymbol{\Phi}$ becomes singular or nearly singular:
\[
  \det \mathbf{A} \to 0 \quad \Rightarrow \quad [a_{ii}^{-1}] \to \infty
\]
so the gradient can explode even when the wavefunction itself is small.


\end{frame}

\begin{frame}[fragile]{Fix: Custom Operation}
    \textbf{Idea:} Use SVD ($A = U\Sigma V^T$) to compute $\nabla \log \det A = A^{-T}$ without explicit inversion (Appendix D).

    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
import torch

class StableLogDet(torch.autograd.Function):
    @staticmethod
    def forward(ctx, A):
        # Decompose A: S are singular values
        U, S, Vh = torch.linalg.svd(A)
        ctx.save_for_backward(U, S, Vh)
        return S.log().sum()

    @staticmethod
    def backward(ctx, g):
        U, S, Vh = ctx.saved_tensors
        # Reconstruct A^{-T} = U * diag(1/S) * Vh
        inv_S = torch.diag_embed(1.0 / S)
        grad_A = U @ inv_S @ Vh
        return g * grad_A
    \end{lstlisting}
\end{frame}

\begin{frame}{Optimizer: Adam vs. AdamW}
    \textbf{Core Difference:} AdamW \textit{decouples} weight decay from the gradient update to fix regularization on adaptive optimizers.

    \vspace{1.5em}

    \textbf{1. Adam (Entangled L2 Regularization)}
    \begin{itemize}
        \item Decay is added to the gradient, so it gets scaled by the adaptive variance.
    \end{itemize}
    \begin{align*}
        g_t &= \nabla \mathcal{L} + \color{red}\lambda \theta_t \\
        \theta_{t+1} &= \theta_t - \text{AdamStep}(g_t)
    \end{align*}

    \vspace{1em}

    \textbf{2. AdamW (Decoupled Weight Decay)}
    \begin{itemize}
        \item Decay is applied directly, bypassing the adaptive scaling mechanism.
    \end{itemize}
    \begin{align*}
        g_t &= \nabla \mathcal{L} \\
        \theta_{t+1} &= \theta_t - \text{AdamStep}(g_t) - \color{green}\eta \lambda \theta_t
    \end{align*}

    \vfill
\end{frame}


\begin{frame}{Keeping the GPU Busy: Batched Energy Evaluation}

\small
\textbf{Naive training issue.}
Initial training evaluated energies step-by-step over MCMC samples.
\[
\text{GPU utilization} \approx 30\%
\]
\textbf{Solution: batched evaluation.}
MCMC samples are reshaped and flattened:
\[
(\text{mc\_steps}, B, n_e, 3)
\;\rightarrow\;
(\text{mc\_steps}\!\times\! B, n_e, 3)
\]
\vspace{0.3cm}
\textbf{Result.}
\[
\text{GPU utilization} \approx 99\%
\]
\end{frame}


\begin{frame}{Results: Convergence Curve}

\begin{columns}
  \begin{column}{0.5\textwidth}

  \begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{img/four_elements.pdf}
  \caption{Convergence Curve}
  \label{fig:convergence_curve}
\end{figure}

  \end{column}
  \begin{column}{0.5\textwidth}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{img/oxigen.pdf}
    \caption{Oxigen Convergence}
  \end{center}
\end{figure}

  \end{column}
\end{columns}

 \end{frame}

\begin{frame}{Results: Energy Estimates}

\begin{figure}
  \centering
  \includegraphics[width=0.90\linewidth]{img/table_energy.pdf}
  \caption{Ground state energies (Ha) for H-O, baseline vs Psiformer}
  \label{fig:energy_estimates}
\end{figure}

\end{frame}

\begin{frame}{Comments}
  \begin{itemize}
    \item Pretraining using external data.
    \item Laplacian Bottleneck
    \item  KFCA Optimizer (Natural Gradient Descent)
    \item Flash Attention
    \item Learning transferability
    \item Scaling Laws
  \end{itemize}
\end{frame}

\begin{frame}{References}
\footnotesize
\begin{thebibliography}{9}

\bibitem{psiformer}
F.~Hermann, Z.~Sch{\"a}tzle, and F.~No{\'e},
\newblock \emph{A Self Attention Ansatz for Ab Initio Quantum Chemistry},
\newblock arXiv:2309.12345, 2023.

\bibitem{ferminet}
J.~Pfau, J.~S.~Spencer, A.~G.~D.~G.~Matthews, and W.~M.~C.~Foulkes,
\newblock \emph{Ab Initio Solution of the Many-Electron Schr{\"o}dinger Equation with Deep Neural Networks},
\newblock Physical Review Research, 2, 033429, 2020.

\bibitem{attention}
A.~Vaswani et al.,
\newblock \emph{Attention Is All You Need},
\newblock Advances in Neural Information Processing Systems (NeurIPS), 2017.

\end{thebibliography}
\end{frame}



\begin{frame}{Thanks!}
I thank the Computer Science Faculty for providing access to GPU resources, in particular NVIDIA GeForce RTX 4080 Super which enabled the training and evaluation of Psiformer models reported in this work.
\begin{figure}
  \centering
  \includegraphics[width=0.40\linewidth]{img/h2.png}
  \end{figure}
\end{frame}

\end{document}
