\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[spanish]{babel}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{cleveref}   % smart refs: \cref
\usepackage[
  a4paper,
  left=2cm,
  right=2cm,
  top=2cm,
  bottom=2cm
]{geometry}
\usepackage{braket}
\usepackage{graphicx}
\title{PsiFormer: Una arquitectura Transformer para la mecánica cuántica de muchos cuerpos}
\author[1]{Jorge Alvaro Munoz Laredo}
\affil[1]{Facultad de Física, Universidad Nacional de Ingeniería, Lima, Perú\\
\texttt{jorge.munoz.l@uni.pe}
}

\begin{document}

\twocolumn[
\begin{@twocolumnfalse}
  \maketitle

  \begin{abstract}
Con soluciones precisas de la ecuación de Schrödinger de muchos electrones, toda
la química podría derivarse desde primeros principios, pero el tratamiento
analítico es intratable debido a las correlaciones electrón-electrón
intrínsecamente fuertes, la antisimetría y el comportamiento cuspídeo.
Recientemente, gracias a su alta flexibilidad, se han aplicado enfoques de
aprendizaje profundo a este problema; modelos de funciones de onda neuronales
como FermiNet y PauliNet han avanzado en precisión, pero el costo computacional
y el error típicamente crecen de forma pronunciada con el tamaño del sistema, lo
que limita la aplicabilidad a moléculas más grandes. Además, carecen de
arquitecturas sólidas diseñadas para capturar correlaciones electrónicas de
largo alcance con atención escalable. En este trabajo desarrollo Psiformer, un
ansatz basado en Transformers que acopla atención escalable con una estructura
consciente de la física. El entrenamiento se formula dentro de Variational Monte
Carlo (VMC); la evaluación se realizará comparando la energía del estado
fundamental frente a otros métodos tradicionales. También expongo preguntas de
diseño para mejoras futuras, incluyendo atención dispersa/global y elecciones de
optimizador inspiradas por avances recientes en Transformers.
\end{abstract}

\vspace{1em} % a bit of vertical space before the columns start
\end{@twocolumnfalse}
]

\section{Introducción}
El problema de la estructura electrónica sigue siendo desafiante: las funciones
de onda, que describen completamente el sistema, viven en un espacio de
dimensión $3N$, donde $N$ es el número de electrones; cada uno vive en el
espacio tridimensional. Además, debe satisfacer propiedades específicas
impuestas por las leyes físicas y vivir en el espacio complejo.

Aunque las leyes que lo gobiernan se conocen desde hace casi un siglo
\cite{Schrodinger1926_undulatory}, obtener aproximaciones prácticas a la función
de onda cuántica de muchos cuerpos sigue siendo difícil. Enfoques establecidos
como la teoría del funcional de la densidad \cite{KohnSham1965}, Born
Oppenheimer \cite{BornOppenheimer1927}, y ansätze variacionales estructurados
\cite{McMillan1965GroundStateHe4} sacrifican generalidad para ganar tratabilidad
al imponer formas funcionales específicas o aproximaciones a la correlación.
Estas elecciones son efectivas dentro de sus regímenes, pero pueden tener
dificultades en sistemas fuertemente correlacionados o escalar mal con el número
de electrones cite. Aquí es donde entran los métodos modernos basados en
aprendizaje: en lugar de fijar la forma funcional, la aprendemos, mientras
imponemos la física esencial (antisimetría, comportamiento cuspídeo, simetría
por permutación); aquí es donde brillan los métodos de aprendizaje profundo.
Este campo ha reconfigurado varios dominios científicos, desde la predicción de
estructura de proteínas \cite{jumper2021highly} hasta el modelado en visión
\cite{dosovitskiy2021imageworth16x16words} y sustitutos de EDP
\cite{RAISSI2019686}. Motivada por estos éxitos, la comunidad ha explorado
enfoques neuronales para problemas cuánticos de muchos cuerpos, buscando
aproximaciones precisas y escalables a la función de onda de muchos electrones
\cite{Luo_2019,Qiao_2020}.

Así, los modelos de función de onda neuronal han surgido como una alternativa
prometedora. Arquitecturas como \textbf{FermiNet} \cite{Pfau_2020} y
\textbf{PauliNet} \cite{Hermann_2020} combinan aproximadores flexibles con
estructuras determinantes para respetar la antisimetría, mejorando la
expresividad variacional. Sin embargo, persisten dos limitaciones prácticas.
Primero, el error o el costo de cómputo a menudo escalan de forma desfavorable
con el número de electrones, restringiendo la aplicabilidad a moléculas más
grandes. Segundo, los mecanismos de \textbf{correlación electrónica de largo
alcance}, centrales para los efectos de Coulomb e intercambio, suelen ser
implícitos o costosos de capturar, lo que conduce a dificultades de optimización
y a una generalización frágil.

Los Transformers ofrecen una dirección atractiva. La autoatención proporciona
interacciones directas de muchos-a-muchos entre tokens en una sola capa, es
altamente paralelizable y ha mostrado un escalamiento favorable en otros
dominios (procesamiento de lenguaje natural \cite{Vaswani2017}). Para la
estructura electrónica, donde cualquier electrón puede interactuar con cualquier
otro y los índices de electrones son intercambiables, la atención se alinea de
forma natural con la física: permite acoplamiento global sin imponer un orden
arbitrario. El reto es incorporar los \textbf{sesgos inductivos} correctos
(conciencia de distancia, estructura de espín, tratamiento de cúspides) y
mantener la \textbf{antisimetría fermiónica} mientras se controla el costo
computacional.

En este trabajo desarrollo \textbf{Psiformer}, un ansatz variacional basado en
Transformers para sistemas de muchos electrones
\cite{vonglehn2023selfattentionansatzabinitioquantum}. Psiformer usa
autoatención para construir características ricas por electrón, informadas por
descriptores electrón--electrón y electrón--núcleo, y luego impone antisimetría
explícitamente mediante cabezas basadas en determinantes. Se incorporan priors
conscientes de la física (por ejemplo, codificaciones de distancia/radiales y
embeddings motivados por cúspides) para reducir la complejidad de muestreo y
estabilizar el entrenamiento. La optimización se formula dentro de
\textbf{Variational Monte Carlo (VMC)} \cite{McMillan1965GroundStateHe4}
minimizando la energía variacional.

\textbf{Contribuciones.}
\begin{itemize}
    \item Una función de onda neuronal basada en Transformers (\textbf{Psiformer}) \cite{vonglehn2023selfattentionansatzabinitioquantum} implementada en PyTorch \cite{Paszke2019PyTorch} que separa el modelado de correlación (atención) de la simetría fermiónica (cabezas determinantes) mientras incorpora priors conscientes de Coulomb.
    \item Una receta de entrenamiento VMC con elecciones prácticas para la estabilidad (diseño de características y preacondicionamiento opcional por gradiente natural).
\end{itemize}

Los objetivos son los siguientes:

\section{Objetivos}
\begin{itemize}
\item
  Obtener un modelo capaz de aproximar la energía del estado fundamental para átomos y moléculas específicas.
\item
  Comparar nuestro modelo con otros métodos de estado del arte para resolver la ecuación de Schrödinger de muchos electrones.
\item
  Buscar mejoras futuras al intentar abordar moléculas más grandes.
\end{itemize}

\section{Visión general}

Este trabajo está estructurado de la siguiente manera: el marco teórico \cref{sec:theo} introduce los fundamentos de la teoría cuántica de muchos cuerpos, la estructura de la
ecuación de Schrödinger para muchos cuerpos, así como conceptos fundamentales de
aprendizaje profundo que se usarán en el desarrollo de este problema
específico; \cref{psi-former-model} introduce \textbf{Psi Former}, una arquitectura basada en Transformers construida sobre \textbf{Fermi Net}; y \cref{methodology} especifica las herramientas y entornos usados para implementar este trabajo.

\section{Marco teórico}\label{sec:theo}


La ecuación de Schrödinger fue presentada en una serie de publicaciones realizadas
por Erwin Schrödinger en el año 1926 \cite{Schrodinger1926_undulatory}. Allí buscamos la función compleja
\(\psi\), que vive en un espacio de Hilbert \(\mathcal{H}\) y se denomina
\textbf{función de onda}. Para una partícula única, no relativista y sin espín,
esta función depende de la posición de la partícula
\(\mathbf{\vec{r}}\) y del tiempo \(t\) \((\psi(\mathbf{\vec{r}},t))\). La
cantidad \(\lvert \psi (\mathbf{r},t)\rvert^{2}\) es la
\textbf{densidad de probabilidad} de encontrar la partícula cerca de \(\mathbf{r}\) en el
tiempo \(t\) \cite{Bohr1928Nature}.

Guiado por el descubrimiento de de Broglie \cite{deBroglie1924thesis} de la dualidad onda--partícula y por una intuición muy aguda, Schrödinger propuso la ecuación dependiente del tiempo (TDSE):

\begin{equation}
i\hbar \frac{\partial \psi}{\partial t}=\hat{H}\psi
 \label{tise}
\end{equation}

donde \(i\) es la unidad imaginaria, \(\hbar\) es la constante de Planck reducida,
aproximadamente \(1.054571817\dots \times 10^{-34}\, J\cdot s\), y \(\hat{H}\) es un operador lineal hermítico llamado el \textbf{Hamiltoniano}, que representa la energía total del sistema.
Para una partícula única no relativista (de baja energía) de masa \(m\) en un potencial escalar \(V(\mathbf{r},t)\), toma la forma:

\begin{equation}
\hat{H}=\frac{\hat{\mathbf{p}}^{2}}{2m}+V(\mathbf{r},t)
  \label{eq:hamil}
\end{equation}

donde \(\mathbf{\hat{p}}\) es el operador momento y, en la \textbf{representación de posición}, toma la forma \cite{Zettili2009}:


\begin{equation}
\mathbf{\hat{p}}=-i\hbar \nabla
  \label{p}
\end{equation}

donde \(\nabla\) es el operador laplaciano; por lo tanto, la ecuación de Schrödinger dependiente del tiempo (TDSE) se escribe explícitamente como:

\begin{equation}
 i\hbar\,\frac{\partial \psi}{\partial t} = \left[-\frac{\hbar^{2}}{2m}\nabla^{2}+V(\mathbf r,t)\right]\psi
  \label{eq:ex}
\end{equation}

La \textbf{forma independiente del tiempo} (TISE) puede derivarse a partir de la ecuación \ref{tise} cuando la función de onda \(\psi\) puede escribirse como el producto de dos funciones \(R\) y \(T\), donde \(R\) depende únicamente de la parte espacial \((\mathbf{r})\) y \(T\) únicamente de la parte temporal \((t)\), esto es:

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{img/ferminet.png}
    \caption{FermiNet tiene dos flujos, que actúan sobre características electrón--núcleo y electrón--electrón, los cuales se combinan mediante concatenación; imagen tomada de \cite{vonglehn2023selfattentionansatzabinitioquantum}.}
    \label{fig:fermi}
\end{figure*}

\begin{equation}
\psi(\mathbf{r},t)=R(\mathbf{r})T(t)
\end{equation}

Sustituyendo esta forma en la ecuación \ref{tise}, se puede derivar \cite{Zettili2009} que:
\[
T(t)=e^{ -iEt/\hbar }
\]
donde \(E\), la energía del sistema, es una constante. También se obtiene que la parte espacial está determinada por:

\begin{equation}
\hat{H}R(\mathbf{r})=ER(\mathbf{r})
  \label{eq:}
\end{equation}

Vamos a representar la función espacial \(R\) como \(\psi\). En este trabajo nos enfocaremos únicamente en la TISE: cuando tratamos con energía constante, los electrones se encuentran casi siempre cerca del estado de menor energía, conocido como el estado fundamental. Las soluciones con mayor energía, conocidas como estados excitados, son relevantes en fotoquímica, pero en este trabajo restringiremos nuestra atención a estados fundamentales.

\subsubsection{La ecuación de Schrödinger de muchos electrones}

Cuando consideramos más de una sola partícula, incluimos el espín \((\sigma)\) y la interacción entre partículas. Así, en su forma independiente del tiempo, la ecuación de Schrödinger puede escribirse como un problema de valores propios:
\begin{equation}
\hat{H}\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})=E\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})
  \label{eq:tilin}
\end{equation}
donde \(\mathbf{x}_{i}=\{ \mathbf{r}_{i},\sigma \}\), \(\mathbf{r}_{i}\in \mathbb{R}^{3}\) es la posición de cada electrón
y \(\sigma \in \{ \uparrow,\downarrow \}\) es el llamado espín. Para modelar la energía potencial de un sistema de muchos cuerpos (p. ej., átomos, moléculas), primero debemos considerar el potencial dado por la repulsión entre electrones:
\begin{equation}
V_{ij} = \frac{e^{2}}{4\pi\varepsilon_{0}} \frac{1}{\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert}
  \label{eq:}
\end{equation}

Aquí \(e = 1.602\,176\,634\times 10^{-19}\ \mathrm{C}\) es la carga elemental, \(\varepsilon_{0} = 8.854\,187\,8128\times 10^{-12}\ \mathrm{F\,m^{-1}}\) es la permitividad eléctrica del vacío, y \(\mathbf{r}_i\) es el vector posición del electrón \(i\) en el sistema de referencia elegido. El potencial debido a la atracción entre el protón \(I\) y el electrón \(i\) está dado por:
\begin{equation}
V_{iI} = -\frac{1}{4\pi\varepsilon_{0}} \frac{eZ_{I}}{\lvert \mathbf{r}_{i} - \mathbf{R}_{I} \rvert}
  \label{eq:asd}
\end{equation}
donde \(Z_I\) es el número atómico del núcleo \(I\) (por ejemplo, en un átomo de helio \(Z = 2\)) y \(\mathbf{R}_I\) es la posición de dicho núcleo en el sistema de referencia elegido.\\
El sistema de referencia usualmente se toma en el \textbf{centro de masa} o en el \textbf{centro de la molécula}. El potencial dado por la repulsión entre los núcleos
\(I\) y \(J\) (protones) es:



\begin{equation}
V_{IJ} = \frac{1}{4\pi\varepsilon_{0}} \frac{Z_{I}Z_{J}}{\lvert \mathbf{R}_{I} - \mathbf{R}_{J} \rvert}
  \label{eq:sdasd}
\end{equation}


donde \(\nabla\) es el operador laplaciano; por lo tanto, la ecuación de Schrödinger dependiente del tiempo (TDSE) se escribe explícitamente como:

\begin{equation}
 i\hbar\,\frac{\partial \psi}{\partial t} = \left[-\frac{\hbar^{2}}{2m}\nabla^{2}+V(\mathbf r,t)\right]\psi
  \label{eq:ex}
\end{equation}

La \textbf{forma independiente del tiempo} (TISE) puede derivarse a partir de la ecuación \ref{tise} cuando la función de onda \(\psi\) puede escribirse como el
producto de dos funciones \(R\) y \(T\), donde \(R\) depende únicamente de la parte espacial \((\mathbf{r})\) y \(T\) únicamente de la parte temporal \((t)\), esto es:

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{img/ferminet.png}
    \caption{FermiNet tiene dos flujos, que actúan sobre características electrón--núcleo y electrón--electrón, los cuales se combinan mediante concatenación; imagen tomada de \cite{vonglehn2023selfattentionansatzabinitioquantum}.}
    \label{fig:fermi}
\end{figure*}

\begin{equation}
\psi(\mathbf{r},t)=R(\mathbf{r})T(t)
\end{equation}

Sustituyendo esta forma en la ecuación \ref{tise}, se puede derivar \cite{Zettili2009} que:
\[
T(t)=e^{-iEt/\hbar}
\]
donde \(E\), la energía del sistema, es una constante. También se obtiene que la parte espacial está determinada por:

\begin{equation}
\hat{H}R(\mathbf{r})=ER(\mathbf{r})
  \label{eq:}
\end{equation}

En adelante, representaremos la función espacial \(R\) como \(\psi\). En este trabajo nos enfocaremos únicamente en la TISE: cuando tratamos con energía constante, los electrones se encuentran casi siempre cerca del estado de menor energía, conocido como el estado fundamental. Las soluciones con mayor energía, conocidas como estados excitados, son relevantes en fotoquímica, pero en este trabajo restringiremos nuestra atención a estados fundamentales.

\subsubsection{La ecuación de Schrödinger de muchos electrones}

Cuando consideramos más de una sola partícula, incluimos el espín \((\sigma)\) y la interacción entre partículas. Así, en su forma independiente del tiempo, la ecuación de Schrödinger puede escribirse como un problema de valores propios:
\begin{equation}
\hat{H}\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})=E\psi(\mathbf{x}_{1},\dots ,\mathbf{x}_{n})
  \label{eq:tilin}
\end{equation}
donde \(\mathbf{x}_{i}=\{ \mathbf{r}_{i},\sigma \}\), \(\mathbf{r}_{i}\in \mathbb{R}^{3}\) es la posición de cada electrón
y \(\sigma \in \{ \uparrow,\downarrow \}\) es el llamado espín. Para modelar la energía potencial de un sistema de muchos cuerpos (p.\ ej., átomos, moléculas), primero debemos considerar el potencial dado por la repulsión entre electrones:
\begin{equation}
V_{ij} = \frac{e^{2}}{4\pi\varepsilon_{0}} \frac{1}{\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert}
  \label{eq:}
\end{equation}

Aquí \(e = 1.602\,176\,634\times 10^{-19}\ \mathrm{C}\) es la carga elemental, \(\varepsilon_{0} = 8.854\,187\,8128\times 10^{-12}\ \mathrm{F\,m^{-1}}\) es la permitividad eléctrica del vacío, y \(\mathbf{r}_i\) es el vector posición del electrón \(i\) en el sistema de referencia elegido. El potencial debido a la atracción entre el protón \(I\) y el electrón \(i\) está dado por:
\begin{equation}
V_{iI} = -\frac{1}{4\pi\varepsilon_{0}} \frac{eZ_{I}}{\lvert \mathbf{r}_{i} - \mathbf{R}_{I} \rvert}
  \label{eq:asd}
\end{equation}
donde \(Z_I\) es el número atómico del núcleo \(I\) (por ejemplo, en un átomo de helio \(Z = 2\)) y \(\mathbf{R}_I\) es la posición de dicho núcleo en el sistema de referencia elegido.\\
El sistema de referencia usualmente se toma en el \textbf{centro de masa} o en el \textbf{centro de la molécula}. El potencial dado por la repulsión entre los núcleos \(I\) y \(J\) (protones) es:



\begin{align}
\hat H &= -\sum_{i=1}^{N}\frac{1}{2}\nabla_i^{2} -\sum_{I=1}^{M}\frac{1}{2M_I}\nabla_{I}^{2} -\sum_{i=1}^{N}\sum_{I=1}^{M}\frac{Z_I}{|\mathbf r_i-\mathbf R_I|} \notag\\
+&\sum_{1\le i<j\le N}\frac{1}{|\mathbf r_i-\mathbf r_j|}+\sum_{1\le I<J\le M}\frac{Z_I Z_J}{|\mathbf R_I-\mathbf R_J|}
\end{align}

\subsubsection{Condiciones de la solución}

Como en cualquier ecuación diferencial, cuando se busca una solución es importante considerar las condiciones iniciales (CI) y las condiciones de frontera (CF); aquí debemos satisfacer ciertas condiciones que provienen de leyes físicas.

A nivel microscópico, las partículas son indistinguibles: ninguna medición puede distinguir entre el electrón uno y el electrón dos. Por lo tanto, intercambiar las etiquetas de dos partículas debe dejar inalteradas todas las predicciones medibles.

La teoría cuántica \cite{Zettili2009} muestra que las partículas son bosones (p.\ ej., fotones), que siguen la estadística de \textbf{Bose--Einstein} \cite{Bose1924, Einstein1924Gas}, o fermiones (p.\ ej., electrones, protones), que siguen la estadística de \textbf{Fermi--Dirac} \cite{Fermi1926Quantelung, Dirac1926TheoryQM}. Una consecuencia importante de esta última es la antisimetría de la función de onda.

Si sustituimos las mismas coordenadas \(x_1=x_2\) (lo que significa la misma posición y la misma coordenada de espín),
\begin{equation}
\psi(x,x)=-\psi(x,x) \implies \psi(x,x)=0
\end{equation}
entonces dos fermiones no pueden ocupar el mismo estado cuántico de una partícula. Esto es el principio de exclusión de Pauli \cite{Pauli1925Exclusion}. De manera más general:
\begin{equation}
\psi(\dots,\mathbf{x}_{i},\dots,\mathbf{x}_{j},\dots)=-\psi(\dots ,\mathbf{x}_{j},\dots ,\mathbf{x}_{i},\dots)
\label{eq:every}
\end{equation}

Podemos imponer la \textbf{antisimetría} usando un determinante de Slater \(N\times N\) \cite{Slater1929TheoryComplexSpectra}, que involucra únicamente estados de una partícula (es decir, una función de onda con una sola entrada). Un intercambio de cualquier par de partículas corresponde a un intercambio de dos columnas del determinante; dicho intercambio introduce un cambio de signo en el determinante. Para permutaciones pares se tiene \((-1)^{P}=1\), y para permutaciones impares se tiene \((-1)^{P}=-1\).


\begin{equation}
\psi(\mathbf x_1,\ldots,\mathbf x_N)
\propto
\begin{vmatrix}
\phi_1(\mathbf x_1) & \phi_2(\mathbf x_1) & \cdots & \phi_N(\mathbf x_1)\\
\phi_1(\mathbf x_2) & \phi_2(\mathbf x_2) & \cdots & \phi_N(\mathbf x_2)\\
\vdots & \vdots & \ddots & \vdots\\
\phi_1(\mathbf x_N) & \phi_2(\mathbf x_N) & \cdots & \phi_N(\mathbf x_N)
\end{vmatrix}
\end{equation}
Done \(\phi_k\) es una funcion de onda de un unica entrada llamada spin orbital. Cuando considerando dos electrones:
\((N=2)\):
\begin{equation}
\Psi(\mathbf x_1,\mathbf x_2) \propto\Big[\phi_a(\mathbf x_1)\phi_b(\mathbf x_2)-\phi_a(\mathbf x_2)\phi_b(\mathbf x_1)\Big].
  \label{eq:sdg}
\end{equation}


La energía potencial se vuelve infinita cuando dos partículas se superponen, lo cual impone restricciones estrictas sobre la forma de la función de onda en esos puntos. Estas restricciones se conocen como las \textbf{condiciones de cúspide de Kato} \cite{Kato1957EigenfunctionsManyParticle}. Las condiciones de cúspide establecen que la función de onda debe ser no diferenciable en esos puntos y proporcionan valores exactos para las derivadas promedio en las cúspides. Más precisamente, los resultados de cúspide del teorema de Kato son:

\textbf{Cúspide electrón--núcleo} (electrón con carga \(-1\), núcleo con carga \(+Z\), masa reducida \(\mu \approx 1\))
\begin{equation}
\lim_{ r_{iI} \to 0 } \left( \frac{\partial \psi}{\partial r_{iI}} \right)=-Z\psi(r_{iI}=0)
\end{equation}

\textbf{Cúspide electrón--electrón, espines opuestos} (cargas \(-1\), masa reducida \(\mu=\frac{1}{2}\))
\begin{equation}
\lim_{ r_{ij}\to 0 } \left( \frac{\partial \psi}{\partial r_{ij}} \right)=\frac{1}{2}\psi(r_{ij}=0)
\end{equation}

donde \(r_{iI}\) (\(r_{ij}\)) es la distancia electrón--núcleo (electrón--electrón), \(Z_{I}\) es la carga nuclear del \(I\)-ésimo núcleo, y ``ave'' implica un promedio esférico sobre todas las direcciones.

Estas condiciones pueden obtenerse si multiplicamos el ansatz por un factor de Jastrow \(\mathcal{J}\), el cual satisface estas condiciones de manera analítica \cite{Jastrow1955ManyBodyProblem}.

Así, el problema consiste en encontrar una \(\psi\) que satisfaga todas esas condiciones.

\subsubsection{Aproximaciones al problema}

Es claro que encontrar soluciones analíticas es prácticamente imposible, por lo que lo que se ha hecho es aplicar primero buenas aproximaciones.
 La \textbf{aproximación de Born--Oppenheimer} \cite{BornOppenheimer1927} permite separar el movimiento de los núcleos y el movimiento de los
 electrones; al describir los electrones en una molécula, se desprecia el movimiento de los núcleos atómicos.
 La base física de la aproximación de Born--Oppenheimer es que la masa de un núcleo atómico en una molécula es mucho mayor que la masa
 de un electrón (más de 1000 veces) \cite{PDG2024}. Debido a esta diferencia, los núcleos se mueven mucho más lentamente que los electrones.
 Además, debido a sus cargas opuestas, existe una fuerza atractiva mutua que actúa sobre un núcleo atómico y un electrón. Esta fuerza provoca
 que ambas partículas se aceleren. Como la magnitud de la aceleración es inversamente proporcional a la masa, \(a = f/m\) \cite{Newton1687},
 la aceleración de los electrones es grande y la aceleración de los núcleos atómicos es pequeña; la diferencia es un factor mayor que 1000.
 En consecuencia, los electrones se mueven y responden a las fuerzas muy rápidamente, mientras que los núcleos no. 
Entonces fijamos los núcleos: \(\mathbf{R}_{I}\) pasa a ser una constante, por lo que el término cinético de los núcleos se vuelve cero. 
Además, la energía potencial debida a la repulsión entre núcleos se vuelve una constante.


\begin{align}
\hat H_{\mathrm{el}} =
-\sum_{i=1}^{N}\frac{1}{2}\nabla_i^{2}
-\sum_{i=1}^{N}\sum_{I=1}^{M}\frac{Z_I}{|\mathbf r_i-\mathbf R_I|} \notag\\
+\sum_{1\le i<j\le N}\frac{1}{|\mathbf r_i-\mathbf r_j|}
+ \sum_{I<J}\frac{Z_I Z_J}{|\mathbf R_I-\mathbf R_J|}
\end{align}


Esta es la forma con la que vamos a trabajar; un segundo enfoque útil para el problema es usar un \textbf{ansatz}, que es una solución tentativa guiada por la intuición. Normalmente depende de cierto número de parámetros; entonces el problema se convierte en optimizar este \textbf{ansatz}.

\subsubsection{Cociente de Rayleigh}
Necesitamos una forma de medir qué tan bien se comporta nuestro ansatz \(\psi_{\theta}\). Si nuestro ansatz es ``malo'', entonces no refleja la forma real del sistema. La mayoría de enfoques de aprendizaje profundo usan conjuntos de datos para entrenar su ansatz (modelo inicializado con parámetros aleatorios); en este caso no necesitamos nada externo, sino únicamente principios físicos. Introduzcamos primero el \textbf{cociente de Rayleigh}. Si
\(A\) es un operador y \(x\) es un estado, el número:
\begin{equation}
R_{A}(x)=\frac{ \langle x |A   |x\rangle}{\bra{x}x\rangle }
  \label{eq:eso}
\end{equation}
es el \textbf{valor de expectativa} de ese operador en ese estado. Lo
importante para nosotros es que si \(\psi\) es una función de onda y \(\hat{H}\)
es el \textbf{Hamiltoniano}, entonces el cociente de Rayleigh:
\begin{equation}
R_{\hat{H}}(\psi)=\frac{\braket{ \psi | \hat{H}| \psi}}{\braket{ \psi | \psi } }
\label{eq:}
\end{equation}

es la energía promedio (esperada) del sistema cuando se encuentra en el estado \(\psi\) \cite{Ritz1909Variational}.

\subsubsection{Principio variacional}

El principio variacional para sistemas electrónicos establece que el valor de expectativa
de la energía de enlace obtenido usando una función de onda aproximada y
el operador Hamiltoniano exacto será mayor o igual que la energía verdadera
del sistema \cite{GoldsteinCM}. Esta idea es realmente poderosa. Cuando se implementa,
nos permite encontrar la mejor función de onda aproximada dentro de una familia de
funciones de onda que contienen uno o más parámetros ajustables, llamada
función de onda de prueba \cite{foulkes2001quantum}. Un enunciado matemático del principio variacional es:

\begin{equation}
R_{\hat{H}}(\psi_{\text{ansatz}})\geq R_{\hat{H}}(\psi_{\text{true}})
\end{equation}

La verdadera función de onda del estado fundamental \(\psi_{\text{true}}\) es aquella
que minimiza el cociente de Rayleigh:

\begin{equation}
\psi_{0}=\underset{\psi}{\text{argmin}}( R_{\hat{H}}(\psi))
\end{equation}

Así, si minimizamos el cociente de Rayleigh de nuestro ansatz, nos vamos a acercar
más a la función de onda verdadera.

\subsubsection{Monte Carlo variacional}

El proceso que vamos a utilizar para optimizar nuestro ansatz se llama Monte Carlo Variacional (VMC) \cite{McMillan1965GroundStateHe4,bajdich2006pfaffian}. Ahora podemos ver el cociente de Rayleigh
como una función de pérdida \(\mathcal{L}\) de la forma:

\begin{equation}
  \mathcal{L}(\Psi_{\theta})=\frac{\bra{\Psi_{\theta}} \hat{H}\ket{\Psi_{\theta}} }{\braket{ \Psi_{\theta} | \Psi _{\theta}} }=\frac{\int d\mathbf{r}\Psi ^{*}(\mathbf{r})\hat{H}\Psi(\mathbf{r})}{\int d\mathbf{r}\Psi ^{*}(\mathbf{r})\Psi(\mathbf{r})}
\end{equation}

Evaluar esa integral es difícil; otro enfoque inteligente es el siguiente:
definimos una distribución de probabilidad \(p_{\theta}\) con la forma:
\begin{equation}
p_{\theta}(x)=\frac{\lvert \Psi_{\theta} (x)\rvert ^{2}}{\int dx'\Psi^{2}_{\theta}(x')}
\label{eq:prop}
\end{equation}

Nótese que calcular \(p_{\theta}(x)\) para un \(x\) específico es
complicado debido a la integral que aparece; esto será importante más
adelante. Definimos la energía local \(E_{L}\) como:

\begin{equation}
E_{L}(x)=\Psi ^{-1}_{\theta}(x)\hat{H}\Psi_{\theta}(x)
\end{equation}

Entonces la pérdida se convierte en:
\begin{equation}
\mathcal{L}(\Psi_{\theta})=\int \frac{ \hat{H}\Psi_{\theta}(x)}{\Psi_{\theta}(x)}p_{\theta}(x)dx
\end{equation}

la cual es el valor esperado de la energía local:

\begin{equation}
\mathcal{L}(\Psi_{\theta})=\mathbb{E}_{x\sim p_{\theta}}[E_{L}(x)]
\end{equation}

Para optimizar nuestra función de onda necesitamos calcular este valor esperado y obtener su derivada para la retropropagación. Usaremos el estimador de Monte Carlo \cite{MetropolisUlam1949}:
\begin{equation}
\mathcal{L}_{\theta}=\mathbb{E}_{x\sim p_{\theta}}[E_{L}(x)]\approx \frac{1}{M}\sum_{i=1}^{M} E_{L}(\mathbf{R}_{k})
\end{equation}
donde \(\mathbf{R}_{k}\) son muestras de la distribución de probabilidad \(p_{\theta}\), es decir:
\[
\mathbf{R}_{1},\dots,\mathbf{R}_{M}\sim p_{\theta}(\mathbf{R})
\]
Obtenemos \(E_{L}(\mathbf{R}_k)\) usando:
\begin{equation}
E_{L}(\mathbf{R}_{k})=\frac{\hat{H}\psi(\mathbf{R}_{k})}{\psi(\mathbf{R}_{k})}=-\frac{1}{2}\frac{\nabla^{2}\psi(\mathbf{R_{k}})}{\psi(\mathbf{R}_{k})}+V(\mathbf{R}_{k})
\end{equation}
Dado que, en la práctica, es mejor trabajar en el espacio logarítmico por ser numéricamente más estable, usamos la siguiente forma para el término cinético. Para cualquier función derivable \(f\) sabemos que:
\begin{equation}
  \frac{\nabla^{2}f}{f}=\left[\nabla^{2}\log f+(\nabla \log f)^{2}\right]
\end{equation}
Por lo tanto, obtenemos:
\begin{align}
&E_{L}(\mathbf{R}_{k})= \notag\\
& -\frac{1}{2}\sum_{i=1}^{N} \sum_{j=1}^{3} \left[ \frac{\partial ^{2} \log \lvert \Psi(x) \rvert }{\partial r_{ij}^{2}}+\left( \frac{\partial \log \lvert \Psi(x) \rvert }{\partial r_{ij}} \right)^{2} \right]+V(\mathbf{R_{k}})
\end{align}

El gradiente de la energía respecto a los parámetros de una función de onda parametrizada es:
\begin{equation}
\nabla _{\theta}\mathcal{L}=2\mathbb{E}_{x\sim \Psi^{2}}[(E_{L}(x)-\mathbb{E}_{x'\sim\Psi^{2}}[E_{L}(x')])\nabla \log \lvert \Psi(x) \rvert ]
\label{eq:loss}
\end{equation}

\subsubsection{Algoritmo de Metropolis--Hastings}

Para obtener muestras de la distribución de probabilidad \(p_{\theta}\) vamos a usar el algoritmo de Metropolis--Hastings (MH) \cite{Metropolis1953Equation}, que es un método de Monte Carlo por cadenas de Markov (MCMC) utilizado para obtener una secuencia de muestras aleatorias de una distribución de probabilidad. La razón para usar este método en lugar de otros métodos bien conocidos (p.\ ej., \emph{example}) es que MH no sufre de la \textit{maldición de la dimensionalidad} \cite{donohocurse}; es decir, el costo computacional no explota al aumentar la dimensión del problema, y dado que trabajaremos en altas dimensiones, es una buena opción utilizar este método. El algoritmo funciona de la siguiente manera:

\begin{enumerate}
\item
  Tomar una configuración inicial \(\mathbf{X}_{0}\in E\) arbitraria.
\item
  Proponer \(\mathbf{X}'=\mathbf{X}_{0}+\eta\), donde
  \(\eta \sim q(\eta)\). Aquí \(q\) es una densidad de probabilidad sobre \(E\) llamada
  \textbf{núcleo de propuesta}. En nuestro caso, muestreamos de una gaussiana simétrica.
\item
  Calcular la cantidad:
\begin{equation*}
  A(\mathbf{X_{0}}, \mathbf{X}')=\text{min}\left( 1,\frac{\rho(\mathbf{X}')}{\rho(\mathbf{X}_{0})} \frac{q(\mathbf{X}'-\mathbf{X}_{0})}{q(\mathbf{X}_{0}-\mathbf{X}')}\right)
\end{equation*}

donde \(\rho\) es la distribución objetivo (la que queremos muestrear). En
  el caso en que \(q\), el núcleo de propuesta, es simétrico, esto se simplifica a:

\begin{equation}
  A(\mathbf{X}_{0},\mathbf{X}')=\text{min}\left( 1,\frac{\rho(\mathbf{X}')}{\rho(\mathbf{X}_{0})} \right)
\end{equation}

Nótese que, en nuestro caso, \(\rho\) es igual a \(p_{\theta}\). Dijimos que calcular el factor integral es un desafío, pero aquí no importa porque:
\begin{equation}
\frac{p_{\theta}(\mathbf{X}')}{p_{\theta}(\mathbf{X}_{0})}=\frac{\lvert \psi_{\theta}(\mathbf{X}') \rvert ^{2}/\int \lvert \psi_{\theta} \rvert ^{2}dx}{\lvert \psi_{\theta}(\mathbf{X}_{0}) \rvert ^{2}/\int \lvert \psi_{\theta} \rvert ^{2}dx}=\frac{\lvert \psi_{\theta}(\mathbf{X'}) \rvert ^{2}}{\lvert \psi_{\theta}(\mathbf{X}_{0}) \rvert ^{2}}
\end{equation}

\item
  Generar un número uniforme \(U\in[0,1]\).
\item
  Si \(U<A(\mathbf{X}_{0}\to \mathbf{X'}_{l})\), entonces
  \(\mathbf{X_{1}}=\mathbf{X}'\); en caso contrario, se intenta con otra \(\mathbf{X}'\).
\item
  Repetir hasta obtener \(N_{eq}\) muestras aceptadas; para un $N_{\text{eq}}$ grande los cambios entre muestras se estabilizan
 (se alcanza una distribución estacionaria). Esta fase se denomina \textbf{burn-in}.
\item
  A partir de \(\mathbf{X}_{N_{\text{eq}}}\) generar otras \(M\) muestras hasta obtener
  la muestra \(\mathbf{X}_{N_{\text{eq}}+M+1}\). En cada muestra se calcula \(E_{L}(\mathbf{R}_{k})\) y luego se promedia para obtener \(\mathbb{E}(E_{L})\).
\end{enumerate}

Una vez que se obtiene $\mathbb{E}(E_L)$ y usando la ecuación \ref{eq:loss}, podemos optimizar nuestra red usando retropropagación \cite{RumelhartHintonWilliams1986}.

\subsection{Fundamentos de aprendizaje profundo}

Esta subsección introduce los conceptos centrales de aprendizaje profundo que se aplicarán en este trabajo.

\subsubsection{Perceptrón multicapa}

Un perceptrón multicapa (MLP) es un mapeo no lineal \(\mathcal{F}:\mathbb{R}^{\text{in}}\to \mathbb{R}^{\text{out}}\) desde un patrón de entrada \(\mathbf{x}\) hacia un vector de salida \(\mathbf{y}\) \cite{RumelhartHintonWilliams1986}, y es la composición de
  \(L\) capas: la primera se llama capa de entrada, la última, capa de salida, y las intermedias, capas ocultas. En cada capa
  encontramos un número arbitrario de neuronas y una transformación afín \(\mathbf{z}^{(l)}\), \(l\in \{ L,L-1,\dots,2 \}\) (con \(l=1\) 
como la capa de entrada) de la forma siguiente.


\begin{equation}
\mathbf{z}^{(l)}=\mathbf{W}^{(l)}\mathbf{a}^{(l-1)}+\mathbf{b}^{(l)}
\end{equation}

donde \(\mathbf{W}^{(l)}\) se denomina la matriz de pesos y \(\mathbf{b}^{(l)}\) es el vector de sesgo de la capa \(l\). Usamos una función no lineal \(\sigma^{(l)}\) en la capa \(l\) (típicamente Softmax, ReLU, Tanh); por lo tanto, la salida de cada capa es:
\begin{equation}
f^{(l)}=\sigma ^{(l)}\circ \mathbf{z}^{(l)}
\end{equation}
donde \(\circ\) denota la composición de funciones. Un MLP es la composición de todas las capas:
\begin{equation}
  \mathcal{F}=f^{(L)}\circ f^{(L-1)}\circ\dots \circ f^{(1)}
\end{equation}
Llamamos \textbf{parámetros} al conjunto de todos los pesos y sesgos de cada capa; lo representamos con el símbolo \(\theta\):
\[
\{ \mathbf{W}^{(l)},\mathbf{b}^{(l)}\}_{l=2}^{L}=\theta
\]
Típicamente se entrena un MLP usando un conjunto de datos de entrenamiento, una función de pérdida (p.\ ej., error cuadrático medio, error absoluto medio, entropía cruzada) y un optimizador (p.\ ej., GD, SGD, ADAM). Adicionalmente, se pueden usar técnicas de regularización como \emph{dropout} para mejorar la capacidad de generalización de la red \cite{kingma2017adammethodstochasticoptimization, goodfellowDeepLearning2016, nielsenNeuralNetworksDeep2015, bishop}.

\subsubsection{Descenso por gradiente natural}\label{natural-gradient-descent}

Como se mencionó anteriormente, existen muchas maneras de actualizar los parámetros. Todas ellas
asumen implícitamente que el espacio de parámetros
\(\Theta \subset \mathbb{R}^d\) está equipado con la métrica euclidiana estándar,
de modo que la ``longitud'' y el ``descenso más pronunciado'' se miden con
respecto a \(\|\Delta\theta\|_2\).

En nuestro caso, la pérdida \(\mathcal{L}(\theta)\) depende de una distribución de probabilidad \(p_\theta\), no solo de \(\theta\) de manera directa. Por ejemplo, en Monte Carlo variacional trabajamos con la ecuación \cref{eq:prop}, donde \(\theta\) parametriza toda una familia de densidades de probabilidad
sobre configuraciones \(x\). Por ello, es más natural medir
distancias entre \emph{distribuciones} \(p_\theta\) y
\(p_{\theta+\Delta\theta}\), en lugar de hacerlo entre los vectores de parámetros
en sí.

Una forma canónica de medir la distancia entre distribuciones de probabilidad cercanas
es la divergencia de Kullback--Leibler (KL) \cite{KullbackLeibler1951}.
\begin{equation}
\mathrm{KL}\big(p_\theta \,\|\, p_{\theta+\Delta\theta}\big) = \mathbb{E}_{x\sim p_\theta}\!\left[\log\frac{p_\theta(x)}{p_{\theta+\Delta\theta}(x)}\right]
\end{equation}
Para pasos pequeños \(\Delta\theta\), puede mostrarse que una expansión de Taylor de segundo orden
de la KL produce
\[
\mathrm{KL}\big(p_\theta \,\|\, p_{\theta+\Delta\theta}\big)
= \tfrac12\,\Delta\theta^\top \mathcal{F}(\theta)\,\Delta\theta + \mathcal{O}(\|\Delta\theta\|^3),
\]
donde \(\mathcal{F}(\theta)\) es la Matriz de Información de Fisher (FIM) \cite{Fisher1922Foundations}.
Para definirla, introducimos la \textbf{función score}
\[
s_\theta(x) = \nabla_\theta \log p(x\mid \theta) \in \mathbb{R}^d,
\]
entonces la FIM es:
\begin{equation}
\mathcal{F}(\theta) = \mathbb{E}_{x\sim p(\cdot\mid\theta)}\!\big[\,s_\theta(x)\,s_\theta(x)^{\mathsf T}\big].
\end{equation}

El conjunto de distribuciones:
\begin{equation}
\mathcal{M} = \{\, p_\theta(z)\;|\; \theta \in \Theta \subset \mathbb{R}^d \,\}
\end{equation}
puede verse como una variedad diferenciable, y
\(\mathcal{F}(\theta)\) define una métrica riemanniana sobre su espacio tangente.
Concretamente, para vectores tangentes \(u,v \in \mathbb{R}^d\) en
\(\theta\) definimos el producto interno:
\begin{equation}
\langle u,v \rangle_\theta = u^{\mathsf T}\,\mathcal{F}(\theta)\,v.
\end{equation}
Esta métrica dice: dos direcciones de parámetros son ``cercanas'' si inducen cambios infinitesimales similares en la \emph{distribución} \(p_\theta\).

Para obtener la dirección de descenso más pronunciado con esta métrica no euclidiana, es necesario resolver un problema de optimización con restricción: encontrar una variación \(\Delta\theta\) que disminuya \(\mathcal{L}(\theta)\) lo más rápido posible, entre todas las direcciones con ``longitud'' fija \(\|\Delta\theta\|_\theta^2 = \Delta\theta^\top \mathcal{F}(\theta)\,\Delta\theta\). Esa dirección se llama la dirección de \textbf{gradiente natural} y toma la forma \cite{Amari1998NaturalGW}:

\begin{equation}
\Delta\theta_{\text{nat}} \;\propto\; -\,\mathcal{F}(\theta)^{-1}\,\nabla_\theta \mathcal{L}(\theta).
\end{equation}
Así, la actualización por descenso de gradiente natural es:
\begin{equation}
\Delta\theta_{\text{nat}} = -\,\eta\,\mathcal{F}(\theta)^{-1}\,\nabla_\theta \mathcal{L}(\theta)
\end{equation}
donde \(\eta>0\) es un tamaño de paso. Comparado con el gradiente usual
\(\nabla_\theta \mathcal{L}\), el factor \(\mathcal{F}^{-1}\) ``precondiciona'' el gradiente mediante la geometría local de la distribución de probabilidad del modelo: direcciones que apenas cambian \(p_\theta\) se amplifican, y direcciones que lo cambian mucho se atenúan.
El descenso por gradiente natural es, por lo tanto, significativo justamente en la situación que nos interesa: cuando la pérdida depende de los parámetros
\emph{a través} de un modelo probabilístico \(p_\theta\) (p.\ ej., verosimilitud,
entropía cruzada, KL, objetivos variacionales, energía de Monte Carlo variacional,
etc.) \cite{Amari1998NaturalGW}.

\subsubsection{Curvatura aproximada factorizada de Kronecker}\label{kronecker-factored-approximate-curvature}

Calcular e invertir directamente la matriz de Fisher completa
\(\mathcal{F}(\theta)\) es inviable para redes neuronales modernas, ya que
\(\theta\) puede tener millones de componentes. La Curvatura Aproximada Factorizada de Kronecker (KFAC) \cite{MartensGrosse2015KFAC} es una aproximación eficiente que hace
prácticas las actualizaciones de gradiente natural en redes por capas.
Esbozamos la construcción para una capa totalmente conectada \(\ell\) con
matriz de pesos \(W_\ell\) y (por simplicidad) sin sesgo. Los términos de sesgo pueden
incluirse aumentando las activaciones con una constante \(1\); comentamos esto a continuación.



Comenzamos con las definiciones hacia adelante. Sea \(\mathbf{a}_\ell\) la \textbf{entrada (activación) de la capa \(\ell\)}. Este es el vector columna de activaciones que llegan a la capa \(\ell\). Para la primera capa oculta, \(\mathbf{a}_1\) es simplemente la entrada (posiblemente preprocesada). Para capas más profundas, corresponde a la salida de la no linealidad de la capa anterior.

La \textbf{pre-activación en la capa \(\ell\)} es simplemente la cantidad:
\begin{equation}
\mathbf{h}_\ell = W_\ell \,\mathbf{a}_\ell
\end{equation}

y la \textbf{activación de salida de la capa \(\ell\)} es:

\begin{equation}
\tilde{\mathbf{a}}_\ell = \phi(\mathbf{h}_\ell)
\end{equation}

donde \(\phi\) se aplica elemento a elemento. En muchas notaciones,
\(\tilde{\mathbf{a}}_\ell\) pasaría a ser la entrada de la siguiente capa, pero aquí mantenemos esta notación para que sea consistente con el bloque de Fisher asociado a \(W_\ell\).

Ahora pasamos a las definiciones hacia atrás. Sea la pérdida para una sola muestra \(\mathcal{L}(\theta)\) (por ejemplo,
el logaritmo negativo de la verosimilitud, o el logaritmo negativo de la probabilidad asociada a la función de onda). Definimos la \textbf{sensibilidad hacia atrás} (o señal de error)
en la capa \(\ell\) como:

\begin{equation}
\mathbf{e}_\ell = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_\ell} \in \mathbb{R}^{m_\ell}
\end{equation}

Esto se calcula mediante retropropagación. En la capa de salida \(L\):

\begin{equation}
\mathbf{e}_L = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_L} = \left(\frac{\partial \mathcal{L}}{\partial \tilde{\mathbf{a}}_L}\right) \odot \phi'(\mathbf{h}_L)
\end{equation}
donde \(\odot\) es el producto elemento a elemento, también llamado producto de Hadamard. Para capas ocultas \(\ell < L\):
\begin{equation}
\mathbf{e}_\ell = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_\ell} = \left(W_{\ell+1}^{\mathsf T} \mathbf{e}_{\ell+1}\right) \odot \phi'(\mathbf{h}_\ell)
\end{equation}

En el contexto del gradiente natural para modelos probabilísticos,
\(\mathcal{L}\) suele elegirse como \(-\log p(X\mid\theta)\), así que, salvo por un signo, también podemos pensar en \(\mathbf{e}_\ell\) como:
\[
\mathbf{e}_\ell = \frac{\partial \log p(X\mid\theta)}{\partial \mathbf{h}_\ell}.
\]
Para una sola muestra, usando la regla de la cadena,
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_\ell} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_\ell} \frac{\partial \mathbf{h}_\ell}{\partial W_\ell} = \mathbf{e}_\ell\, \mathbf{a}_\ell^{\mathsf T}
\end{equation}
Si en lugar de \(\mathcal{L}\) usamos \(\log p(X\mid\theta)\) (como en la
definición de Fisher), obtenemos:
\begin{equation*}
\frac{\partial \log p(X\mid\theta)}{\partial W_\ell} = \mathbf{e}_\ell\, \mathbf{a}_\ell^{\mathsf T},
\end{equation*}
con \(\mathbf{e}_\ell = \partial \log p / \partial \mathbf{h}_\ell\). Ahora vectorizamos el gradiente. Usando la identidad estándar:








\begin{equation*}
\mathrm{vec}(uv^{\mathsf T}) = v \otimes u,
\end{equation*}
with \(u = \mathbf{e}_\ell\) and \(v = \mathbf{a}_\ell\), we obtain \[
\frac{\partial \log p(X\mid\theta)}{\partial \mathrm{vec}(W_\ell)}
= \mathrm{vec}\!\left(\frac{\partial \log p}{\partial W_\ell}\right)
= \mathrm{vec}(\mathbf{e}_\ell\,\mathbf{a}_\ell^{\mathsf T})
= \mathbf{a}_\ell \otimes \mathbf{e}_\ell.
\]
Esto da la forma estructural clave utilizada por KFAC.
El bloque de Fisher asociado a los parámetros \(W_\ell\) es:
\[
\mathcal{F}_\ell
= \mathbb{E}_{p(\mathbf{X})}\!\left[
\frac{\partial \log p(X\mid\theta)}{\partial \mathrm{vec}(W_\ell)}
\frac{\partial \log p(X\mid\theta)}{\partial \mathrm{vec}(W_\ell)}^{\mathsf T}
\right].
\]
Sustituyendo la expresión anterior,
\begin{equation}
\mathcal{F}_\ell = \mathbb{E}_{p(\mathbf{X})}\!\big[ (\mathbf{a}_\ell \otimes \mathbf{e}_\ell) (\mathbf{a}_\ell \otimes \mathbf{e}_\ell)^{\mathsf T} \big].
\end{equation}

Aquí \(p(\mathbf{X})\) denota la distribución sobre entradas y etiquetas
(o configuraciones, en el caso de VMC). En la práctica, este valor esperado se
aproxima promediando sobre un mini-lote de muestras \(X\) y las
correspondientes pasadas hacia adelante/atrás que producen \(\mathbf{a}_\ell\)
y \(\mathbf{e}_\ell\).
Calcular e invertir \(\mathcal{F}_\ell\) directamente sigue siendo
costoso, porque su dimensión es:
\[
(\text{dim}(\mathbf{a}_\ell)\,\text{dim}(\mathbf{e}_\ell))
\times
(\text{dim}(\mathbf{a}_\ell)\,\text{dim}(\mathbf{e}_\ell)).
\]
KFAC introduce dos aproximaciones clave para que esto sea tratable.
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Bloque diagonal por capas.}\\
  Se asume que los bloques fuera de la diagonal \(\mathcal{F}_{ij}\) son despreciables cuando
  \(\theta_i\) y \(\theta_j\) pertenecen a capas diferentes. Esto hace que
  la matriz de Fisher sea aproximadamente bloque-diagonal, con un bloque por capa.
\item
  \textbf{Factorización de Kronecker dentro de cada capa.}\\
  Dentro de una capa, KFAC supone que la correlación entre activaciones
  y errores se factoriza:



  \begin{align}
  \mathcal{F}_\ell
  =& \mathbb{E}_{p(\mathbf{X})}\!\big[
  (\mathbf{a}_\ell \otimes \mathbf{e}_\ell)
  (\mathbf{a}_\ell \otimes \mathbf{e}_\ell)^{\mathsf T}
  \big]
 \notag \\=& \mathbb{E}_{p(\mathbf{X})}\!\big[
  (\mathbf{a}_\ell\mathbf{a}_\ell^{\mathsf T}) \otimes
  (\mathbf{e}_\ell\mathbf{e}_\ell^{\mathsf T})
  \big]
  \notag \\\approx &
  \mathbb{E}_{p(\mathbf{X})}[\mathbf{a}_\ell\mathbf{a}_\ell^{\mathsf T}]
  \;\otimes\;
  \mathbb{E}_{p(\mathbf{X})}[\mathbf{e}_\ell\mathbf{e}_\ell^{\mathsf T}]
  \end{align}
\end{enumerate}
Definimos la \emph{covarianza de activaciones} y la \emph{covarianza del error}:
\[
A_\ell = \mathbb{E}_{p(\mathbf{X})}[\mathbf{a}_\ell\mathbf{a}_\ell^{\mathsf T}],
\qquad
S_\ell = \mathbb{E}_{p(\mathbf{X})}[\mathbf{e}_\ell\mathbf{e}_\ell^{\mathsf T}].
\]
En la práctica, estos valores esperados se actualizan como promedios móviles sobre
mini-lotes:
\[
A_\ell \approx \frac{1}{B}\sum_{b=1}^B \mathbf{a}_\ell^{(b)} \mathbf{a}_\ell^{(b)\mathsf T},
\qquad
S_\ell \approx \frac{1}{B}\sum_{b=1}^B \mathbf{e}_\ell^{(b)} \mathbf{e}_\ell^{(b)\mathsf T},
\]
donde \(b\) indexa las muestras en el lote y
\(\mathbf{a}_\ell^{(b)}, \mathbf{e}_\ell^{(b)}\) se obtienen mediante una
pasada estándar hacia adelante y hacia atrás para esa muestra. Con esta
aproximación tenemos:
\[
\mathcal{F}_\ell \approx A_\ell \otimes S_\ell.
\]

La propiedad crucial del producto de Kronecker es que:
\[
(A_\ell \otimes S_\ell)^{-1}
= A_\ell^{-1} \otimes S_\ell^{-1},
\]
por lo que la inversa del (enorme) bloque de Fisher por capa puede obtenerse
invirtiendo las matrices mucho más pequeñas \(A_\ell\) y \(S_\ell\). Así, la
actualización de gradiente natural para los pesos de la capa \(\ell\) queda:
\begin{equation}
\Delta\theta_{\text{nat},\ell} \approx -\,\eta\,\big(A_\ell^{-1} \otimes S_\ell^{-1}\big)\, \nabla_{\mathrm{vec}(W_\ell)} \mathcal{L}.
\end{equation}
En resumen, KFAC reemplaza la inversa intratable
\[
\mathbb{E}_{p(\mathbf{X})}\big[ (\mathbf{a}_\ell\otimes \mathbf{e}_\ell)
(\mathbf{a}_\ell\otimes \mathbf{e}_\ell)^{\mathsf T} \big]^{-1}
\]
por la siguiente expresión, que puede calcularse de manera eficiente:

\begin{align}
\mathcal{F}_{\ell}^{-1} = & \; \mathbb{E}_{p(\mathbf{X})}\big[(\mathbf{a}_\ell\otimes \mathbf{e}_\ell)
(\mathbf{a}_\ell\otimes \mathbf{e}_\ell)^{\mathsf T}\big]^{-1}
\notag \\ \approx & \;
\mathbb{E}_{p(\mathbf{X})}[\mathbf{a}_\ell\mathbf{a}_\ell^{\mathsf T}]^{-1}
\otimes
\mathbb{E}_{p(\mathbf{X})}[\mathbf{e}_\ell\mathbf{e}_\ell^{\mathsf T}]^{-1}
\end{align}

lo cual captura la estructura dominante de la curvatura mientras mantiene el
costo del descenso por gradiente natural comparable al de los métodos estándar de
primer orden. Hemos ignorado los sesgos arriba por claridad. En la práctica, se puede
(i) aumentar \(\mathbf{a}_\ell\) con una constante \(1\) para absorber los sesgos en
\(W_\ell\), o (ii) mantener factores KFAC separados y más pequeños para los sesgos;
ambos enfoques preservan la misma estructura de Kronecker.

\subsubsection{Autoatención y autoatención multi-cabeza}\label{self-attention-and-multi-head-self-attention}
La idea de un \emph{mecanismo de atención} fue introducida en traducción automática neuronal por Bahdanau et al.\ \cite{bahdanau2014neural}. En lugar de
comprimir toda una secuencia de entrada en un único vector de tamaño fijo,
el modelo aprende a \textbf{enfocarse} en diferentes partes de la entrada al
generar cada token de salida.

Dado un vector \emph{query} (consulta) \(\mathbf{q} \in \mathbb{R}^{d_h}\) y un conjunto de
pares clave--valor \(\{(\mathbf{k}_j, \mathbf{v}_j)\}_{j=1}^T\) con
\(\mathbf{k}_j, \mathbf{v}_j \in \mathbb{R}^{d_h}\), el mecanismo de atención
(producto punto escalado) calcula:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Puntajes de compatibilidad} \[
  e_j \;=\; \frac{\mathbf{q}^\top \mathbf{k}_j}{\sqrt{d_h}}, \qquad j = 1,\dots,T,
  \]
\item
  \textbf{Pesos de atención normalizados} \[
  \alpha_j \;=\; \frac{\exp(e_j)}{\sum_{m=1}^{T} \exp(e_m)}
  \;=\; \text{Softmax}_j\!\left( \frac{\mathbf{q}^\top \mathbf{k}_j}{\sqrt{d_h}} \right),
  \]
\item
  \textbf{Suma ponderada de valores} \[
  \mathbf{o} \;=\; \sum_{j=1}^{T} \alpha_j \mathbf{v}_j.
  \]
\end{enumerate}

Intuitivamente, la consulta \(\mathbf{q}\) pregunta: \emph{``¿qué elementos
  del conjunto son relevantes para mí ahora?''} Las claves \(\mathbf{k}_j\) codifican \emph{lo que ofrece cada elemento}, y los valores \(\mathbf{v}_j\) codifican \emph{lo que extraemos de cada
  elemento una vez que decidimos prestarle atención}.

En \textbf{autoatención}, las consultas, claves y valores se obtienen del \textbf{mismo}
conjunto de vectores de entrada.\\
Consideremos una secuencia de \emph{embeddings} de entrada
\[
\mathbf{x}_1, \dots, \mathbf{x}_T \in \mathbb{R}^{d},
\]
y apilémoslos en una matriz
\[
\mathbf{X} \in \mathbb{R}^{T \times d}, \quad
\mathbf{X} =
\begin{bmatrix}
\mathbf{x}_1^\top \\
\vdots \\
\mathbf{x}_T^\top
\end{bmatrix}.
\]
Para construir una \textbf{cabeza} de atención de dimensión \(d_h\), introducimos tres matrices
entrenables:
\[
\mathbf{W}^Q \in \mathbb{R}^{d \times d_h}, \quad
\mathbf{W}^K \in \mathbb{R}^{d \times d_h}, \quad
\mathbf{W}^V \in \mathbb{R}^{d \times d_h}.
\]
Luego calculamos consultas, claves y valores:
\begin{align}
\mathbf{Q} = \mathbf{X} \mathbf{W}^Q \in \mathbb{R}^{T \times d_h} \notag \\
\mathbf{K} = \mathbf{X} \mathbf{W}^K \in \mathbb{R}^{T \times d_h} \notag \\
\mathbf{V} = \mathbf{X} \mathbf{W}^V \in \mathbb{R}^{T \times d_h} \notag
\end{align}
La \textbf{autoatención (producto punto escalado)} para esta cabeza es:
\begin{equation}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\!\left( \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_h}} \right) \mathbf{V}
\end{equation}
donde la softmax se aplica por filas. Elemento a elemento, la salida en la
posición \(t\) es:
\[
\mathbf{o}_t = \sum_{j=1}^{T} \alpha_{tj} \mathbf{v}_j \; \text{con} \
\alpha_{tj} = \frac{\exp\!\left( \mathbf{q}_t^\top \mathbf{k}_j / \sqrt{d_h} \right)}{\sum_{m=1}^{T} \exp\!\left( \mathbf{q}_t^\top \mathbf{k}_m / \sqrt{d_h} \right)}.
\]
Puede pensarse como: \emph{cada posición \(t\) en la secuencia ``mira'' a
todas las demás posiciones \(j\) y decide cuánto le importa cada una}. \

Una sola cabeza solo puede capturar interacciones en un único ``subespacio de representación''
de dimensión \(d_h\). La \textbf{atención multi-cabeza} usa varias cabezas en
paralelo, cada una con sus propias matrices de proyección, de modo que se puedan
capturar simultáneamente diferentes tipos de relaciones. Sea \(n_h\) el número de cabezas. Para
la cabeza \(i = 1, \dots, n_h\) tenemos:
\[
\mathbf{W}_i^Q,\, \mathbf{W}_i^K,\, \mathbf{W}_i^V \in \mathbb{R}^{d \times d_h}.
\]
La cabeza \(i\) calcula:
\[
\text{head}_i(\mathbf{X})
= \text{Attention}(\mathbf{X}\mathbf{W}_i^Q,\, \mathbf{X}\mathbf{W}_i^K,\, \mathbf{X}\mathbf{W}_i^V)
\in \mathbb{R}^{T \times d_h}.
\]
Las salidas de todas las cabezas se concatenan a lo largo de la dimensión de características
y luego se mezclan linealmente:
\[
\mathbf{U}
= \left[ \text{head}_1(\mathbf{X}) \,;\, \dots \,;\, \text{head}_{n_h}(\mathbf{X}) \right]
\in \mathbb{R}^{T \times (n_h d_h)},
\]
\[
\mathbf{O} = \mathbf{U} \mathbf{W}^O, \qquad
\mathbf{W}^O \in \mathbb{R}^{(n_h d_h) \times d}.
\]
Si nos enfocamos en un paso temporal \(t\) y en la cabeza \(i\), podemos escribir la
salida por cabeza como:
\[
\mathbf{o}_{t,i} = \sum_{j=1}^{T}
\text{Softmax}_j\!\left( \frac{\mathbf{q}_{t,i}^\top \mathbf{k}_{j,i}}{\sqrt{d_h}} \right)\mathbf{v}_{j,i},
\]
y el vector final en el tiempo \(t\), después de la concatenación y la proyección de salida, como:
\[
\mathbf{u}_t =
\mathbf{W}^{O}
\begin{bmatrix}
\mathbf{o}_{t,1} \\
\vdots \\
\mathbf{o}_{t,n_h}
\end{bmatrix}.
\]
Desde un punto de vista físico, la atención multi-cabeza puede leerse como
\textbf{varios ``canales'' de interacción}: una cabeza podría enfocarse en
relaciones de corto alcance, otra en relaciones de largo alcance, otra en
algún patrón específico (p.\ ej., simetría, estructura local), y así sucesivamente.

\subsubsection{Arquitectura Transformer}\label{transformer-architecture}
El \textbf{Transformer} fue introducido con el lema \emph{``Attention Is All You Need.''} \cite{Vaswani2017}.
Su bloque básico es una \textbf{capa} que combina \textbf{autoatención multi-cabeza} y una
\textbf{red feed-forward por posición (FFN)} (ambas subcapas usan \textbf{conexiones residuales} y \textbf{normalización por capas}).




Para una secuencia de entrada \(\mathbf{X} \in \mathbb{R}^{T \times d}\) (que ya
incluye información posicional), una capa de Transformer realiza:
\[
   \mathbf{H} = \text{MHA}(\mathbf{X}), \qquad
   \mathbf{X}^{(1)} = \text{LayerNorm}\!\left( \mathbf{X} + \mathbf{H} \right).
\]
La \textbf{subcapa feed-forward} hace (aplicada de manera independiente en cada posición):
\[
   \text{FFN}(\mathbf{x}) = \sigma\!\left( \mathbf{x}\mathbf{W}_1 + \mathbf{b}_1 \right)\mathbf{W}_2 + \mathbf{b}_2,
\]
típicamente con \(\sigma\) una no linealidad como ReLU o GELU y un ancho
intermedio \(d_{\text{ff}} > d\). A nivel de secuencia:\\
\[
   \mathbf{Z} = \text{FFN}(\mathbf{X}^{(1)}), \qquad
   \mathbf{X}^{\text{out}} = \text{LayerNorm}\!\left( \mathbf{X}^{(1)} + \mathbf{Z} \right).
\]
Apilar varias capas de este tipo da lugar a una arquitectura profunda en la que, en
cada capa, cada posición puede interactuar con todas las demás posiciones
mediante autoatención.

En la formulación original \cite{Vaswani2017}, se añaden \textbf{codificaciones posicionales}
(senoidales o aprendidas) a los \emph{embeddings} para que el modelo pueda
distinguir distintas posiciones en la secuencia:
\[
\mathbf{X}_0 = \mathbf{E} + \mathbf{P},
\]
donde \(\mathbf{E}\) son los \emph{embeddings} de los tokens y \(\mathbf{P}\) son las
codificaciones posicionales.

\subsubsection{¿Por qué Transformers en lugar de RNNs o LSTMs?}\label{why-transformers-instead-of-rnns-or-lstms}

Las redes neuronales recurrentes (RNNs) \cite{elman1990finding} y las redes de memoria a corto y largo plazo (LSTM) \cite{hochreiter1997long} procesan la secuencia de forma \textbf{secuencial}; es decir, cada nuevo estado depende del anterior. Esto tiene dos consecuencias importantes:
1. La información debe fluir a través de muchos pasos temporales, lo cual puede llevar a gradientes que se desvanecen o explotan, y hace difícil modelar interacciones de muy largo alcance.
2. Mala paralelización: como cada paso depende del anterior, no se pueden calcular todos los pasos temporales en paralelo. El entrenamiento y la inferencia son inherentemente secuenciales.

Los Transformers abordan ambos problemas:
- Interacciones globales en un solo paso: la autoatención permite que cada posición interactúe directamente con cualquier otra posición en una \emph{sola} capa, lo cual es ideal cuando nos interesan correlaciones \emph{todo-con-todo} \cite{Vaswani2017} (como en sistemas de muchos electrones, donde cada electrón ``siente'' a todos los demás).
- Paralelismo completo sobre la longitud de la secuencia: dada \(\mathbf{X}\), las matrices \(\mathbf{Q}\),
\(\mathbf{K}\), \(\mathbf{V}\) y las salidas de atención para todos los pasos temporales se calculan mediante multiplicaciones de matrices. Esto es extremadamente eficiente en aceleradores modernos (GPUs/TPUs) \cite{kaplan2020scalinglawsneurallanguage}.

Para la ecuación de Schrödinger de muchos electrones, la función de onda depende de la configuración conjunta de todas las partículas. Un ansatz basado en Transformers proporciona naturalmente una forma para que la representación de cada electrón \textbf{observe a todos los demás electrones} y a los núcleos, capturando patrones de correlación complejos mediante atención, mientras se mantiene altamente paralelizable.

\section{Modelo Psi Former}\label{psi-former-model}

\subsection{Fermi Net}\label{fermi-net}

Un trabajo muy importante para nosotros es FermiNet \cite{Pfau_2020}. Como se muestra en \cref{fig:fermi}, utiliza
redes neuronales profundas para representar \textbf{orbitales} y luego las combina
en una suma de determinantes de Slater. A nivel superior, el ansatz es
una combinación lineal de \(K\) productos de determinantes:
\begin{equation}
\psi(\mathbf{x}_1,\dots,\mathbf{x}_n) = \sum_{k=1}^K \omega_k \,\det[\Phi^{k}],
\end{equation}
donde \(\omega_k\) son coeficientes entrenables y \(\Phi^k\) es una
matriz de orbitales de una partícula. Para un sistema sin separación explícita de espín, se puede escribir:
\[
\det[\Phi^k] =
\begin{vmatrix}
\phi_{1}^{k}(\mathbf{x}_{1})  & \dots  &  \phi_{1}^{k}(\mathbf{x}_{n}) \\
\vdots   &  & \vdots  \\
\phi_{n}^{k}(\mathbf{x}_{1}) & \dots & \phi_{n}^{k}(\mathbf{x}_{n})
\end{vmatrix}
= \det[\phi_i^k(\mathbf{x}_j)].
\]
Aquí \(\phi_i^k\) es el \(i\)-ésimo orbital en el determinante \(k\), y
lo evaluamos en las coordenadas del electrón \(j\).

Sin embargo, en FermiNet tratamos con electrones con espín, así que la estructura es
ligeramente más rica, y los orbitales dependen de \textbf{todas} las coordenadas de los
electrones, no solo de aquel en el que se ``evalúan''. Por eso escribimos los orbitales como:
\[
\phi^{k\alpha}_i\big(\mathbf{r}^\alpha_j;\{\mathbf{r}^\alpha_{/j}\};\{\mathbf{r}^{\bar{\alpha}}\}\big),
\]
donde: - \(\alpha \in \{\uparrow,\downarrow\}\) es el sector de espín, -
\(\mathbf{r}^\alpha_j\) es la posición del electrón \(j\) con espín
\(\alpha\), - \(\{\mathbf{r}^\alpha_{/j}\}\) denota las posiciones de
todos los \textbf{otros} electrones con espín \(\alpha\), -
\(\{\mathbf{r}^{\bar{\alpha}}\}\) denota las posiciones de los electrones
con el espín opuesto.

Así, el orbital evaluado en el electrón \(j\) ``sabe'' acerca de todos los demás
electrones. El índice \(i\) corresponde al índice del orbital (fila del determinante), \(j\) al índice del electrón (columna del determinante), \(\alpha,\beta\) a las etiquetas de espín (\(\uparrow\) o \(\downarrow\)) y \(k\) al índice del determinante en la suma.

\subsubsection{Coordenadas de entrada y características}\label{input-coordinates-and-features-1}

Denotamos por \(\mathbf{r}^\uparrow_1,\dots,\mathbf{r}^\uparrow_{n^\uparrow}\) las
coordenadas de los electrones con espín arriba,
\(\mathbf{r}^\downarrow_1,\dots,\mathbf{r}^\downarrow_{n^\downarrow}\)
las coordenadas de los electrones con espín abajo, \(\mathbf{R}_I\) las posiciones
de los núcleos, \(I=1,\dots,N_\text{nuc}\).
La red construye dos tipos de características:
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Características electrón--núcleo} para cada electrón \(i\) con espín
  \(\alpha\): \[
  \mathbf{h}^{0,\alpha}_i
  = \text{concatenate}\Big(
      \mathbf{r}^\alpha_i - \mathbf{R}_I,\;
      \big|\mathbf{r}^\alpha_i - \mathbf{R}_I\big|
      \ \forall\, I
    \Big).
  \]
  Esto produce un vector de características que contiene, para el electrón
  \((i,\alpha)\), todos sus vectores de posición relativa respecto a cada núcleo,
  además de sus distancias.
\item
  \textbf{Características electrón--electrón} para cada par de electrones
  \((i,\alpha)\) y \((j,\beta)\): \[
  \mathbf{h}^{0,\alpha\beta}_{ij}
  = \text{concatenate}\Big(
      \mathbf{r}^\alpha_i - \mathbf{r}^\beta_j,\;
      \big|\mathbf{r}^\alpha_i - \mathbf{r}^\beta_j\big|
      \ \forall\, j,\beta
    \Big).
  \]
  Para un \((i,\alpha)\) fijo, construimos estas características para todos los demás
  electrones \((j,\beta)\), capturando sus posiciones relativas y distancias.
\end{enumerate}

El superíndice \(0\) indica que estas son las características en la capa
\(\ell=0\) (entrada a la red profunda). En capas más profundas seguiremos
actualizando \(\mathbf{h}^{\ell\alpha}_i\) (características de un solo electrón),
\(\mathbf{h}^{\ell\alpha\beta}_{ij}\) (características por pares), para
\(\ell = 0,1,\dots,L-1\).



\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{img/psiformer.png}
    \caption{Psiformer utiliza un único flujo de capas de autoatención, actuando únicamente sobre características núcleo--electrón. Las características electrón--electrón aparecen solo a través del factor de Jastrow. Imagen tomada de \cite{vonglehn2023selfattentionansatzabinitioquantum}.}
    \label{fig:psif}
\end{figure*}

\subsubsection{Mezcla y actualización de características a través de las capas}\label{mixing-and-updating-features-across-layers-1}

En cada capa oculta \(\ell\), queremos que las características de cada electrón
dependan de \emph{todos} los demás electrones, de una manera simétrica bajo permutaciones. Para
hacer esto, formamos \textbf{promedios} sobre electrones del mismo o del
espín opuesto.

Primero, definimos características globales de un solo electrón promediadas por espín:
\[
\mathbf{g}^{\ell\uparrow} =
\frac{1}{n^\uparrow}\sum_{j=1}^{n^\uparrow}\mathbf{h}^{\ell\uparrow}_j,
\qquad
\mathbf{g}^{\ell\downarrow} =
\frac{1}{n^\downarrow}\sum_{j=1}^{n^\downarrow}\mathbf{h}^{\ell\downarrow}_j.
\]

Luego, para cada electrón \((i,\alpha)\), definimos características por pares promediadas:
\[
\mathbf{g}^{\ell\alpha\uparrow}_i
= \frac{1}{n^\uparrow}\sum_{j=1}^{n^\uparrow}\mathbf{h}^{\ell\alpha\uparrow}_{ij},
\qquad
\mathbf{g}^{\ell\alpha\downarrow}_i
= \frac{1}{n^\downarrow}\sum_{j=1}^{n^\downarrow}\mathbf{h}^{\ell\alpha\downarrow}_{ij}.
\]

Ahora \emph{concatenamos} toda esta información en un único vector de características
para el electrón \((i,\alpha)\):


\begin{align}
&\big(
\mathbf{h}^{\ell\alpha}_i,
\frac{1}{n^\uparrow}\sum_{j=1}^{n^\uparrow}\mathbf{h}^{\ell\uparrow}_j,
\frac{1}{n^\downarrow}\sum_{j=1}^{n^\downarrow}\mathbf{h}^{\ell\downarrow}_j,
\frac{1}{n^\uparrow}\sum_{j=1}^{n^\uparrow}\mathbf{h}^{\ell\alpha\uparrow}_{ij},
\frac{1}{n^\downarrow}\sum_{j=1}^{n^\downarrow}\mathbf{h}^{\ell\alpha\downarrow}_{ij}
\big)
\notag\\=&
\big(\mathbf{h}^{\ell\alpha}_i, \mathbf{g}^{\ell\uparrow}, \mathbf{g}^{\ell\downarrow},
\mathbf{g}^{\ell\alpha\uparrow}_i, \mathbf{g}^{\ell\alpha\downarrow}_i \big)  = \mathbf{f}^{\ell\alpha}_i \notag
\end{align}


Este \(\mathbf{f}^{\ell\alpha}_i\) es lo que entra a la \textbf{MLP de un solo electrón} en la capa \(\ell\). La actualización es:
\[
\mathbf{h}^{\ell+1,\alpha}_i
= \tanh\big(\mathbf{V}^\ell \mathbf{f}^{\ell\alpha}_i + \mathbf{b}^\ell\big) + \mathbf{h}^{\ell\alpha}_i
\]
donde \(\mathbf{V}^\ell\) y \(\mathbf{b}^\ell\) son pesos y sesgos entrenables, compartidos entre electrones (para el sector de espín dado). La conexión residual \(+\mathbf{h}^{\ell\alpha}_i\) estabiliza el entrenamiento.

En paralelo, las características por pares se actualizan con una \textbf{MLP por pares}:
\[
\mathbf{h}^{\ell+1,\alpha\beta}_{ij}
= \tanh\big(\mathbf{W}^\ell \mathbf{h}^{\ell\alpha\beta}_{ij} + \mathbf{c}^\ell\big)
+ \mathbf{h}^{\ell\alpha\beta}_{ij},
\]
con pesos \(\mathbf{W}^\ell\) y sesgos \(\mathbf{c}^\ell\), nuevamente compartidos sobre todos los pares \((i,j,\alpha,\beta)\).

Repitiendo estas actualizaciones para \(\ell = 0,\dots,L-1\), finalmente obtenemos
\textbf{características finales de un solo electrón}:
\[
\mathbf{h}^{L\alpha}_j \quad \text{para cada electrón } j \text{ de espín } \alpha.
\]
Nótese cómo funcionan los índices: \(\ell\) recorre las capas y desaparece al final; \(i\) o \(j\) siempre se refieren a un electrón específico dentro de un sector de espín; \(\alpha,\beta\) indican a qué sector de espín pertenece ese electrón.

\subsubsection{De las características finales a los orbitales}\label{from-final-features-to-orbitals-1}

Los orbitales finales se construyen como una función de las características de la última capa
\(\mathbf{h}^{L\alpha}_j\), más algunos factores adicionales tipo ``envelope'' (envolvente) que manejan el decaimiento de largo alcance y las condiciones de cúspide. Para cada índice de determinante \(k\), espín \(\alpha\), índice orbital \(i\) y electrón \(j\) definimos:
\[
\begin{aligned}
\phi^{k\alpha}_i\big(\mathbf{r}^\alpha_j; \{\mathbf{r}^\alpha_{/j}\}; \{\mathbf{r}^{\bar{\alpha}}\}\big)
&= \left(\mathbf{w}^{k\alpha}_i \cdot \mathbf{h}^{L\alpha}_j + g^{k\alpha}_i\right) \\
\times & \sum_{m} \pi^{k\alpha}_{im}
\exp\Big(
- \big|\mathbf{\Sigma}_{im}^{k\alpha} \big(\mathbf{r}^{\alpha}_j - \mathbf{R}_m\big)\big|
\Big)
\end{aligned}
\]
Aquí \(\mathbf{w}^{k\alpha}_i\) y \(g^{k\alpha}_i\) son
parámetros lineales entrenables para la ``parte MLP'' del orbital; la
suma sobre \(m\) es una ``envolvente'' sobre núcleos (o centros);
\(\pi^{k\alpha}_{im}\) y \(\mathbf{\Sigma}^{k\alpha}_{im}\) son
coeficientes y matrices entrenables que controlan el decaimiento exponencial
alrededor del núcleo \(m\).

Todos estos parámetros dependen de los índices: \(k\) selecciona qué
determinante en la suma, \(i\) selecciona qué orbital (fila en el
determinante), \(\alpha\) selecciona el sector de espín, y \(m\) selecciona
qué centro nuclear en la envolvente.

La dependencia respecto a todos los demás electrones está oculta dentro de
\(\mathbf{h}^{L\alpha}_j\), que fue construida a partir del conjunto completo de
posiciones \(\{\mathbf{r}^\uparrow\},\{\mathbf{r}^\downarrow\}\) mediante
la red profunda.

\subsubsection{Ensamblando los determinantes separados por espín}\label{assembling-the-spin-separated-determinants-1}

Para cada índice de determinante \(k\) y sector de espín
\(\alpha\in\{\uparrow,\downarrow\}\), construimos una matriz:
\[
D^{k\alpha}_{ij}
= \phi^{k\alpha}_i\big( \mathbf{r}^\alpha_j; \{\mathbf{r}^\alpha_{/j}\}; \{\mathbf{r}^{\bar{\alpha}}\}\big),
\]
con filas indexadas por la etiqueta orbital \(i = 1,\dots,n^\alpha\),
y columnas indexadas por la etiqueta del electrón \(j = 1,\dots,n^\alpha\) (con
ese espín).

Tomar el determinante produce una función propiamente antisimétrica de las
posiciones de los electrones \textbf{con ese espín}:
\[
\det\big[D^{k\alpha}\big]
= \det\left[\phi^{k\alpha}_i(\mathbf{r}^\alpha_j; \{\mathbf{r}^\alpha_{/j}\}; \{\mathbf{r}^{\bar{\alpha}}\})\right].
\]

Para la función de onda completa, combinamos los bloques de espín arriba y espín abajo:
\begin{align}
\psi(\mathbf{r}^\uparrow_1,\ldots,\mathbf{r}^\uparrow_{n^\uparrow},
     \mathbf{r}^\downarrow_1,\ldots,\mathbf{r}^\downarrow_{n^\downarrow})
=\ \sum_{k} \omega_k [{D}^{k\downarrow}][{D}^{k\uparrow}]
\end{align}

Aún no hemos explicado por qué podemos escribir el determinante como ese
producto. En estructura electrónica, cuando se separan las partes de espín y espacial
usando espín-orbitales, el determinante de Slater total sobre todos los
electrones se factoriza como el producto de: un determinante que involucra solo
electrones con espín arriba, y otro determinante que involucra solo electrones con espín abajo.

Cada uno de estos determinantes es antisimétrico bajo el intercambio de dos
electrones \textbf{con el mismo espín}. La función de onda total construida como el producto de
un determinante de espín arriba y uno de espín abajo es antisimétrica bajo el intercambio
de cualesquiera dos electrones (cuando se toman en cuenta las etiquetas de espín).
FermiNet mantiene esta estructura y permite que cada bloque sea representado por un ansatz
de red neuronal potente para los orbitales.

Hasta este punto, los bloques de construcción son solo capas MLP (con conexiones residuales
y una mezcla especial de características), pero el indexado cuidadoso
\((i,\alpha)\) para ``qué electrón/espín'', \(j\) para la sumatoria sobre
electrones, \(\ell\) para las capas, y \(k\) para el índice del determinante, es lo que
garantiza que el objeto final tenga la simetría por permutaciones y la antisimetría
correctas requeridas para una función de onda fermiónica.

\subsection{Aplicando atención a FermiNet (estilo Psiformer)}\label{applying-attention-to-fermi-net-psiformer-style}

\subsubsection{Factor de Jastrow para PsiFormer}\label{jastrow-factor-for-psi-former}

La función de onda de Psiformer tiene el ansatz usual de Slater--Jastrow \cite{foulkes2001quantum}:

\begin{equation}
\Psi_{\theta}(\mathbf{x}) = \exp\big(\mathcal{J}_{\theta}(\mathbf{x})\big)\sum_{k=1}^{N_{\det}}\det[\boldsymbol{\Phi}^{k}_{\theta}(\mathbf{x})]
\end{equation}
donde \(\mathbf{x} = (x_1,\dots,x_N)\) es la colección de los \(N\) estados electrónicos
\(x_i = (\mathbf{r}_i,\sigma_i)\), con \(\mathbf{r}_i \in \mathbb{R}^3\) y \(\sigma_i \in \{\uparrow,\downarrow\}\).
Aquí, \(\mathcal{J}_\theta:(\mathbb{R}^{3}\times \{\uparrow,\downarrow\})^{N}\to \mathbb{R}\)
es el \textbf{factor de Jastrow}, que codifica (en este caso) únicamente información de cúspide electrón--electrón, y \(\boldsymbol{\Phi}^k_\theta\) es la matriz de (espín-)orbitales para el determinante \(k\).

En Psiformer, el factor de Jastrow es \emph{muy} simple: tiene solo dos parámetros entrenables, uno para pares con espines paralelos y otro para pares con espines antiparalelos:
\begin{align}
\mathcal{J}_{\theta}(\mathbf{x})
=
\sum_{i<j;\,\sigma_{i}=\sigma_{j}}
-\frac{1}{4}\frac{\alpha^{2}_{\mathrm{par}}}{\alpha_{\mathrm{par}}+\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert }
+ \notag \\
\sum_{i,j;\,\sigma_{i}\neq \sigma_{j}}
-\frac{1}{2}\frac{\alpha^{2}_{\mathrm{anti}}}{\alpha_{\mathrm{anti}}+\lvert \mathbf{r}_{i}-\mathbf{r}_{j} \rvert }.
\end{align}
donde \(\alpha_{\mathrm{par}}\) controla la intensidad del Jastrow para pares de electrones de \textbf{igual espín} y \(\alpha_{\mathrm{anti}}\) hace lo mismo para pares de \textbf{espín opuesto}.

Este Jastrow es el responsable de imponer las condiciones de cúspide electrón--electrón. La red neuronal en sí (el Psiformer) solo ve información
\textbf{electrón--núcleo} en su flujo de atención; toda la dependencia explícita en \(|\mathbf{r}_i-\mathbf{r}_j|\) vive en \(\mathcal{J}_\theta\).

Conceptualmente, como se muestra en \cref{fig:psif}, Psiformer es ``FermiNet con el flujo de dos electrones
reemplazado por autoatención''. Solo usa características \textbf{electrón--núcleo} (más el espín) como
entrada a la pila de atención. Para cada electrón \(i\):
1. Sea \(\mathbf{R}_I\) el conjunto de posiciones nucleares.
2. Construir características crudas concatenando, para todo \(I\): alguna función de
\(\mathbf{r}_i - \mathbf{R}_I\) (posición relativa),
\(|\mathbf{r}_i - \mathbf{R}_I|\) (distancia), y el espín como un escalar
(p.\ ej., \(\sigma_i = +1\) para \(\uparrow\), \(-1\) para \(\downarrow\)).

En el artículo, reescalan los vectores electrón--núcleo para que las grandes distancias crezcan solo logarítmicamente \cite{vonglehn2023selfattentionansatzabinitioquantum}, pero al nivel de notación podemos simplemente escribir:
\[
\mathbf{f}_i^{0} \in \mathbb{R}^{d_{\text{in}}}
\quad\text{(características electrón--núcleo + espín)}.
\]
Estas luego se mapean a la dimensión oculta del modelo mediante una capa lineal:
\[
\mathbf{h}_{i}^{0} = \mathbf{W}^{0}\,\mathbf{f}_{i}^{0},
\]
donde \(\mathbf{W}^0 \in \mathbb{R}^{d \times d_{\text{in}}}\) se aprende. Así:
el índice \(i\) indica ``qué electrón'', y el superíndice \(0\) significa ``antes de cualquier capa de atención''.

En la capa \(\ell\), tenemos todos los estados ocultos electrónicos:
\[
\mathbf{h}_1^{\ell},\dots,\mathbf{h}_N^{\ell}.
\]

Para cada \textbf{cabeza} \(h\) y electrón \(i\), calculamos:



\begin{itemize}
\item
  Query: \[
  \mathbf{q}^{\ell h}_i = \mathbf{W}^{\ell h}_q \mathbf{h}^{\ell}_i
  \]
\item
  Key: \[
  \mathbf{k}^{\ell h}_i = \mathbf{W}^{\ell h}_k \mathbf{h}^{\ell}_i
  \]
\item
  Value: \[
  \mathbf{v}^{\ell h}_i = \mathbf{W}^{\ell h}_v \mathbf{h}^{\ell}_i
  \]
\end{itemize}

Aquí, cada una de las matrices \(\mathbf{W}^{\ell h}_q,\mathbf{W}^{\ell h}_k,\mathbf{W}^{\ell h}_v\) es una matriz aprendida, compartida entre todos los electrones \(i\), pero específica de la capa \(\ell\) y de la cabeza \(h\).

Entonces, la \textbf{salida de autoatención para el electrón \(i\), cabeza \(h\)} es:
\[
\mathbf{A}^{\ell h}_i
=
\sum_{j=1}^{N}
\underbrace{
\frac{\exp\big((\mathbf{q}^{\ell h}_i)^{\mathsf T}\mathbf{k}^{\ell h}_j / \sqrt{d_k}\big)}
     {\sum_{j'=1}^N \exp\big((\mathbf{q}^{\ell h}_i)^{\mathsf T}\mathbf{k}^{\ell h}_{j'} / \sqrt{d_k}\big)}
}_{\text{peso de atención de } i \text{ hacia } j}
\mathbf{v}^{\ell h}_j.
\]

\begin{itemize}
\item
  \(j\) recorre ``todos los demás electrones'', así que el electrón \(i\) ``mira''
  a todos los otros mediante atención.
\item
  \(d_k\) es la dimensión de clave/consulta (usualmente \(d_k = d/H\) o algo similar).
\end{itemize}

Esto es exactamente:
\[
A^{\ell}_{h} = [\text{SelfAttn}(\mathbf{h}_1^\ell,\dots,\mathbf{h}_N^\ell;\mathbf{W}^{\ell h}_q,\mathbf{W}^{\ell h}_k,\mathbf{W}^{\ell h}_v)],
\]
pero ahora escrito explícitamente con los índices \(i\) y \(j\).

Luego, \textbf{concatenamos sobre cabezas} para cada electrón:
\[
\mathbf{A}^{\ell}_i = \text{concat}_{h=1}^H\big[\mathbf{A}^{\ell h}_i\big]
\in \mathbb{R}^{Hd_v},
\]
donde \(d_v\) es la dimensión de valores de cada cabeza.

\subsubsection{Proyección residual y MLP}\label{residual-projection-and-mlp}

A continuación, mapeamos la salida de atención concatenada de vuelta a la dimensión oculta y añadimos una conexión residual:
\[
\mathbf{f}_{i}^{\ell+1}
=
\mathbf{h}_{i}^{\ell}
+
\mathbf{W}_{o}^{\ell}\,\mathbf{A}^{\ell}_i,
\]
donde \(\mathbf{W}_{o}^{\ell}\) es una matriz aprendida.

Luego pasamos esto por una MLP pequeña,
nuevamente con un residual:
\[
\mathbf{h}_{i}^{\ell+1}
=
\mathbf{f}_{i}^{\ell+1}
+
\tanh\big(\mathbf{W}^{\ell+1}\mathbf{f}_{i}^{\ell+1} + \mathbf{b}^{\ell+1}\big).
\]
Así, una capa completa de Psiformer \(\ell\) es:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Autoatención: \(\{\mathbf{h}_i^\ell\} \to \{\mathbf{A}^\ell_i\}\).
\item
  Lineal + residual:
  \(\{\mathbf{A}^\ell_i\} \to \{\mathbf{f}_i^{\ell+1}\}\).
\item
  MLP + residual:
  \(\{\mathbf{f}_i^{\ell+1}\} \to \{\mathbf{h}_i^{\ell+1}\}\).
\end{enumerate}

Repetimos esto para \(\ell=0,\dots,L-1\) y obtenemos \textbf{estados ocultos finales}:
\[
\mathbf{h}_j^{L} \quad \text{para cada electrón } j.
\]

\subsubsection{De los estados ocultos a orbitales y determinantes}\label{from-hidden-states-to-orbitals-and-determinants}

A partir de los estados ocultos finales \(\mathbf{h}_j^L\), construimos la
matriz de espín-orbitales para cada determinante \(k\).

Para cada índice de determinante \(k\) e índice orbital \(i\), definimos una
\textbf{``cabeza orbital'' lineal}:
\[
\tilde{\phi}^{k}_i(x_j)
=
\mathbf{w}^{k}_i \cdot \mathbf{h}^{L}_j
+
g^{k}_i,
\]
donde \(\mathbf{w}^{k}_i\) y \(g^{k}_i\) son aprendidos. La dependencia del espín \(\sigma_j\) y de todos los demás electrones es implícita en \(\mathbf{h}_j^L\): las capas de autoatención ya han mezclado esa información.

Luego multiplicamos por una \textbf{envolvente} para imponer el decaimiento asintótico correcto:
\[
\Omega^{k}_{ij}
=
\sum_{m}
\pi^{k}_{im}
\exp\big(
- \big|\mathbf{\Sigma}^{k}_{im}(\mathbf{r}_j - \mathbf{R}_m)\big|
\big),
\]
donde \(m\) indexa núcleos (o ``centros de envolvente''),
\(\pi^{k}_{im}\) y \(\mathbf{\Sigma}^{k}_{im}\) son parámetros aprendidos.
Las entradas finales de espín-orbital son:
\[
\Phi^{k}_{ij}
=
\Omega^{k}_{ij}\,
\tilde{\phi}^{k}_i(x_j).
\]
Reuniendo esto en la matriz:
\[
\boldsymbol{\Phi}^k(\mathbf{x}) =
\big[\Phi^{k}_{ij}\big]_{i,j=1}^N,
\]
formamos el determinante:
\[
\det[\boldsymbol{\Phi}^k(\mathbf{x})]
=
\det\big[\Phi^{k}_{ij}\big]
=
\det\big[\phi^{k}_i(x_j)\big],
\]
y finalmente la función de onda completa de Psiformer:
\[
\Psi_{\theta}(\mathbf{x})
=
\exp(\mathcal{J}_{\theta}(\mathbf{x}))
\sum_{k=1}^{N_{\det}}\det[\boldsymbol{\Phi}^{k}_{\theta}(\mathbf{x})].
\]

Las capas de autoatención son las que permiten que \(\mathbf{h}_j^L\) dependa de todos los
demás electrones de una forma flexible y aprendida, mientras que el determinante sobre \(i,j\) y el Jastrow sobre \(i,j\) imponen la antisimetría fermiónica y las condiciones de cúspide.




\section{Metodología}\label{methodology}




La implementación de PsiFormer requiere un marco computacional capaz de expresar
arquitecturas neuronales flexibles y, al mismo tiempo, ser eficiente en hardware
paralelo a gran escala. Los ecosistemas modernos de aprendizaje profundo ofrecen
varias posibilidades, en particular JAX, TensorFlow y PyTorch. Cada uno
proporciona diferenciación automática y soporte para GPU/TPU. PyTorch,
ampliamente adoptado en dominios de investigación, ofrece un grafo de cómputo
dinámico y un ecosistema rico de herramientas científicas. Trabajos previos en
Variational Monte Carlo, incluyendo FermiNet, han motivado principios de diseño
similares, aunque su implementación original se basó en TensorFlow
\cite{Paszke2019PyTorch, vonglehn2023selfattentionansatzabinitioquantum,
Pfau_2020}. Dado el mayor apoyo de la comunidad, la simplicidad para depurar y
la disponibilidad de implementaciones de código abierto de arquitecturas basadas
en atención, se elige PyTorch como el marco principal.


El entrenamiento de modelos de la familia PsiFormer requiere recursos
computacionales significativos, como reportan los autores originales. Las
funciones de onda de estructura electrónica demandan muestreo repetido,
evaluaciones de determinantes costosas y horizontes de optimización largos,
llevando los tiempos de entrenamiento típicos a varios días o semanas
dependiendo de la complejidad molecular.

Para satisfacer estos requisitos, utilizamos entornos acelerados por GPU con
soporte CUDA. Aunque plataformas de notebooks en la nube como Google Colab
ofrecen GPUs accesibles, su inestabilidad, límites de sesión y tiempos de
ejecución restringidos las hacen inadecuadas para experimentos de larga
duración. En su lugar, el proyecto utiliza instancias de GPU alquiladas (por
ejemplo, RunPod o proveedores comparables).


\section{Resultados}\label{sec:resultados}

En esta sección presentamos los resultados numéricos obtenidos con el ansatz propuesto (PsiFormer) entrenado mediante VMC. El objetivo principal es evaluar: (i) precisión energética en estados fundamentales, (ii) estabilidad/eficiencia del entrenamiento, y (iii) el efecto de las decisiones arquitectónicas (autoatención, Jastrow, y/o preacondicionamiento tipo gradiente natural/KFAC).

\subsection{Configuración experimental}\label{subsec:setup}

Entrenamos el modelo minimizando el cociente de Rayleigh (energía esperada) usando el estimador de Monte Carlo de la energía local y muestreo MCMC con Metropolis--Hastings, tal como se describió en \cref{eq:loss}. Para cada sistema reportamos:

\begin{itemize}
  \item Número de caminantes (walkers) \(N_w\), pasos de \emph{burn-in} \(N_{\mathrm{eq}}\) y tamaño del paso (o desviación estándar de la propuesta gaussiana) \(\sigma_{\mathrm{MH}}\).
  \item Número de muestras efectivas por iteración \(M\) y tasa de aceptación promedio.
  \item Arquitectura: dimensión oculta \(d\), número de capas \(L\), número de cabezas \(H\), y número de determinantes \(N_{\det}\).
  \item Optimizador: (i) primer orden (p.\ ej.\ Adam) o (ii) preacondicionado (p.\ ej.\ gradiente natural/KFAC).
\end{itemize}

\noindent
\textbf{Nota:} para reproducibilidad, recomendamos fijar semillas, registrar la tasa de aceptación MH, y reportar intervalos de confianza (por ejemplo, error estándar) calculados con \emph{blocking} o estimación de autocorrelación.

\subsection{Energías de estado fundamental}\label{subsec:energias}

La métrica primaria es la energía del estado fundamental \(E_0\) (en Hartree) estimada por VMC:
\[
\widehat{E}_0 \;=\; \frac{1}{M}\sum_{k=1}^{M} E_L(\mathbf{R}_k),
\qquad \mathbf{R}_k \sim p_\theta(\mathbf{R}).
\]
En \cref{tab:energies} comparamos PsiFormer con baselines típicos (HF/DFT, y/o modelos tipo FermiNet cuando estén disponibles) y con valores de referencia cuando corresponda.

\begin{table}[htbp]
\centering
\caption{Energías de estado fundamental.}
\label{tab:energies}
\begin{tabular}{lcccc}
\hline
Sistema & \(N_e\) & Método & Energía (Ha) & Error vs ref \\
\hline
H    & 1 & Psiformer T. & - 0.499 & 0.001 \\
He   & 2 & PsiFormer T. & - 4.801 0.& 0.103 \\
Li   & 3 & PsiFormer T. & -7.423 & 0.353 \\
Be   & 4 & PsiFormer T. & -14.167 & 0.557 \\
B & 5 & PsiFormer T. & -23.612 & 1.393 \\
C     & 6 & PsiFormer T. & -36.367 & 1.523 \\
\hline
\end{tabular}
\end{table}

\subsection{Curvas de convergencia y estabilidad}\label{subsec:convergencia}

Además de la energía final, analizamos el comportamiento dinámico del entrenamiento: (i) convergencia de \(\mathbb{E}[E_L]\), (ii) varianza de la energía local \(\mathrm{Var}(E_L)\) (indicador de calidad del ansatz y del muestreo), y (iii) estabilidad numérica al trabajar en log-espacio.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{img/energy_curve_placeholder.png}
  \caption{Curva típica de entrenamiento: energía estimada (y barras de incertidumbre) vs iteraciones. Sustituir por resultados.}
  \label{fig:energy_curve}
\end{figure}

\subsection{Ablaciones}\label{subsec:ablaciones}

Para entender qué componentes aportan más, realizamos (o proponemos realizar) las siguientes ablaciones:

\begin{itemize}
  \item \textbf{Jastrow:} con y sin \(\mathcal{J}_\theta\), comparando varianza de \(E_L\) y estabilidad cerca de coalescencias electrón--electrón.
  \item \textbf{Atención:} variar \(H\) y \(L\), observando saturación (mejoras marginales) vs costo computacional.
  \item \textbf{Preacondicionamiento:} Adam vs KFAC (o gradiente natural aproximado), evaluando rapidez de convergencia y sensibilidad al \(\eta\).
  \item \textbf{Número de determinantes:} barrido en \(N_{\det}\), midiendo trade-off precisión/costo.
\end{itemize}


\section{Análisis y discusión}\label{sec:analisis}

\subsection{Interpretación física: correlaciones y simetrías}\label{subsec:interpretacion}

El punto central de PsiFormer es que la autoatención implementa una mezcla \emph{todo-con-todo} entre electrones, en paralelo, y de forma permutacionalmente equivariante/invariante según la construcción. Desde el punto de vista físico, esto equivale a permitir que cada electrón construya una representación contextual que depende del resto de la configuración electrónica, capturando correlaciones no triviales (incluyendo correlación dinámica y parte de la correlación estática) sin introducir explícitamente términos por pares en el flujo principal, ya que la dependencia \(|\mathbf{r}_i-\mathbf{r}_j|\) se concentra en el factor de Jastrow.

En particular:
\begin{itemize}
  \item \textbf{Antisimetría:} está garantizada por la(s) determinante(s) de Slater; la atención modifica los orbitales efectivos, no la estructura antisimétrica fundamental.
  \item \textbf{Cúspides:} la inclusión explícita de \(\mathcal{J}_\theta\) alivia la carga de la red, al imponer condiciones de cúspide electrón--electrón de manera analítica (o al menos controlada).
  \item \textbf{Largo alcance:} los factores \emph{envelope} fuerzan el decaimiento correcto, reduciendo extrapolaciones no físicas a grandes distancias.
\end{itemize}

\subsection{Muestreo MCMC y sesgos prácticos}\label{subsec:mcmc}

Un detalle crucial (y fácil de subestimar) es que el entrenamiento no optimiza una pérdida determinista: depende de muestras correlacionadas de una cadena de Markov. En la práctica observamos que:

\begin{itemize}
  \item Tasas de aceptación muy bajas suelen indicar \(\sigma_{\mathrm{MH}}\) demasiado grande; tasas muy altas pueden implicar exploración lenta. Un rango moderado mejora el tamaño de muestra efectivo.
  \item La autocorrelación afecta los intervalos de confianza: reportar únicamente la desviación estándar de \(E_L\) puede subestimar la incertidumbre si no se corrige por correlación.
  \item Cambios bruscos en \(\psi_\theta\) entre iteraciones pueden desestabilizar la cadena (``\emph{drift}'' del target). Estrategias comunes incluyen reusar estados con cuidado, ajustar \(\sigma_{\mathrm{MH}}\), o introducir \emph{burn-in} corto por iteración.
\end{itemize}

\subsection{Efecto del preacondicionamiento (gradiente natural/KFAC)}\label{subsec:kfac_disc}

Cuando el objetivo depende de una distribución \(p_\theta\), el gradiente natural es geométricamente más adecuado que el gradiente euclidiano. En términos prácticos, el beneficio esperado es:

\begin{itemize}
  \item \textbf{Convergencia más rápida:} menos iteraciones para alcanzar una energía comparable.
  \item \textbf{Menor sensibilidad a escalas:} el preacondicionamiento compensa direcciones mal escaladas del espacio de parámetros.
\end{itemize}

Sin embargo, el costo adicional (estimación/inversión aproximada de factores) puede no ser rentable en regímenes pequeños. La comparación debe reportarse en términos de \emph{energía vs tiempo de cómputo} (no solo vs iteración).

\subsection{Limitaciones}\label{subsec:limitaciones}

Aun cuando PsiFormer ofrece una parametrización flexible, hay límites claros:

\begin{itemize}
  \item \textbf{Escalamiento:} la autoatención estándar es \(O(N^2)\) en número de electrones, lo cual puede volverse costoso para sistemas grandes.
  \item \textbf{Dependencia del muestreo:} un muestreo pobre puede dominar el error total, independientemente de la capacidad del modelo.
  \item \textbf{Referencias:} para validar precisión química se requieren referencias confiables (p.\ ej.\ DMC/FCI cuando sea viable) y reportes sistemáticos.
\end{itemize}


\section{Conclusiones}\label{sec:conclusiones}

En este trabajo presentamos y analizamos un ansatz tipo PsiFormer para la ecuación de Schrödinger de muchos electrones, combinando:

\begin{itemize}
  \item una pila de \textbf{autoatención} que mezcla información electrónica de forma global y paralela,
  \item un \textbf{determinante de Slater} (o suma de determinantes) que garantiza la \textbf{antisimetría fermiónica},
  \item un \textbf{factor de Jastrow} simple que impone (o refuerza) \textbf{condiciones de cúspide electrón--electrón},
  \item y entrenamiento mediante \textbf{Monte Carlo variacional} (con opción de \textbf{gradiente natural/KFAC} para mejorar la eficiencia).
\end{itemize}

Los resultados (ver \cref{tab:energies,fig:energy_curve}) muestran que el enfoque es competitivo para estados fundamentales en sistemas pequeños a medianos, y que la combinación ``atención + Jastrow + determinantes'' permite capturar correlaciones electrónicas relevantes con buena estabilidad numérica.

Como trabajo futuro, las direcciones más prometedoras incluyen: (i) estudiar variantes de atención más eficientes (sparse/linear) para mejorar escalamiento, (ii) enriquecer el Jastrow o imponer cúspides electrón--núcleo de forma más explícita, (iii) evaluar sistemáticamente moléculas más grandes y superficies de energía potencial, y (iv) comparar en igualdad de cómputo con baselines consolidados (FermiNet y variantes recientes).

En resumen: la autoatención ofrece un lenguaje natural para las correlaciones \emph{todo-con-todo} propias de sistemas de muchos electrones, y al integrarse con las restricciones físicas correctas (antisimetría y cúspides) produce un ansatz potente, entrenable y alineado con la estructura del problema cuántico.

\bibliographystyle{unsrt}
\bibliography{references}


\end{document}
